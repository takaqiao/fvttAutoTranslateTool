import json
import os
import time
import re
import shutil
import hashlib
import concurrent.futures
import pandas as pd
from datetime import datetime
from threading import Lock
from tqdm import tqdm
from google import genai
from google.genai import types

# ================= é…ç½®åŒºåŸŸ =================

API_KEY = "YOUR_API_KEY_HERE"

# 1. æ ¸å¿ƒæ–‡ä»¶é…ç½®
SOURCE_EN_JSON_PATH = "pf2e-beginner-box.adventures.json"
TARGET_JSON_PATH = "pf2e-beginner-box_CN.json"

# 2. æœ¯è¯­è¡¨é…ç½®
GLOBAL_GLOSSARY_PATH = "æœ¯è¯­è¯‘åå¯¹ç…§è¡¨.csv" 
LOCAL_GLOSSARY_EXPORT_PATH = "æœ¯è¯­è¡¨_æœ¬åœ°æå–.csv"

# 3. æ€§èƒ½ä¸é‡è¯•
TARGET_RPM = 950
MAX_WORKERS = 64
MAX_RETRIES = 5

# 4. æ—¥å¿—ä¸ç¼“å­˜
REPORT_XLSX_PATH = "ç¿»è¯‘å®¡æŸ¥æŠ¥å‘Š.xlsx"
PROCESS_LOG_PATH = "è¿è¡Œæ—¥å¿—.txt"
DROPPED_LOG_PATH = "æœ¯è¯­ä¸¢å¼ƒæ—¥å¿—.txt"
HISTORY_FILE_PATH = "translation_history.json" # ç¼“å­˜æ–‡ä»¶
BACKUP_DIR = "backups"

# ç›®æ ‡å­—æ®µ
TARGET_KEYS = {"name", "description", "text", "label", "caption", "value", "unidentifiedName", "tokenName", "publicnotes", "publicNotes"}
SPECIAL_CONTAINERS = {"notes", "folders"}

MODEL_ID = 'gemini-3-flash-preview' 
ENABLE_CODE_PROTECTION = True 

# ===========================================

client = genai.Client(api_key=API_KEY)
log_lock = Lock()

report_data = {"New": [], "Fixed": [], "Kept": []}
process_log_buffer = []
history_cache = set()
new_history_entries = set()

SAFETY_SETTINGS = [
    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
]

def write_process_log(msg):
    with log_lock:
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        process_log_buffer.append(f"[{timestamp}] {msg}")

# === å…¨é‡å¤‡ä»½ç³»ç»Ÿ (V26 æ ¸å¿ƒ) ===

def backup_existing_files():
    """å¤‡ä»½æ‰€æœ‰å…³é”®æ–‡ä»¶ï¼Œå»ºç«‹ç‰ˆæœ¬å¿«ç…§"""
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # éœ€è¦å¤‡ä»½çš„æ–‡ä»¶æ¸…å•
    files_to_backup = [
        TARGET_JSON_PATH,           # åŠæˆå“ JSON
        REPORT_XLSX_PATH,           # ä¸Šä¸€æ¬¡çš„æŠ¥å‘Š
        LOCAL_GLOSSARY_EXPORT_PATH, # ä¸Šä¸€æ¬¡æå–çš„æœ¯è¯­
        HISTORY_FILE_PATH           # ç¼“å­˜æ–‡ä»¶
    ]
    
    backup_count = 0
    for file_path in files_to_backup:
        if os.path.exists(file_path):
            filename = os.path.basename(file_path)
            # ä¸ºäº†æ–¹ä¾¿æ’åºï¼Œç»Ÿä¸€å‘½åæ ¼å¼: æ—¶é—´æˆ³_æ–‡ä»¶å.bak
            backup_name = f"{timestamp}_{filename}.bak"
            backup_path = os.path.join(BACKUP_DIR, backup_name)
            
            try:
                shutil.copy2(file_path, backup_path)
                backup_count += 1
            except Exception as e:
                print(f"âŒ å¤‡ä»½å¤±è´¥ {filename}: {e}")
                
    if backup_count > 0:
        print(f"ğŸ“¦ å·²å»ºç«‹å…¨é‡å¿«ç…§: {timestamp} (å¤‡ä»½äº† {backup_count} ä¸ªæ–‡ä»¶)")

# === ç¼“å­˜ç³»ç»Ÿ ===

def get_content_hash(en_text, cn_text):
    if not en_text: en_text = ""
    if not cn_text: cn_text = ""
    raw = f"{en_text}::{cn_text}"
    return hashlib.md5(raw.encode('utf-8')).hexdigest()

def load_history():
    if os.path.exists(HISTORY_FILE_PATH):
        try:
            with open(HISTORY_FILE_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data)
        except: return set()
    return set()

def save_history():
    final_history = history_cache.union(new_history_entries)
    try:
        with open(HISTORY_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump(list(final_history), f)
        print(f"ğŸ’¾ ç¼“å­˜æ›´æ–°: {len(final_history)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

# === åŸºç¡€å·¥å…· ===

class RateLimiter:
    def __init__(self, rpm):
        self.interval = 60.0 / rpm
        self.last_dispatch_time = 0
    def wait_for_slot(self):
        now = time.time()
        next_slot = self.last_dispatch_time + self.interval
        wait_time = next_slot - now
        if wait_time > 0: time.sleep(wait_time)
        self.last_dispatch_time = time.time()

class CodeProtector:
    def __init__(self):
        self.patterns = [
            re.compile(r'(@[a-zA-Z0-9]+\[[^\]]*\])'),
            re.compile(r'(\[\[.*?\]\])'),
            re.compile(r'(<[^>]+>)'),
            re.compile(r'(&[a-zA-Z0-9#]+;)'),
        ]
    def mask(self, text):
        if not text: return text, {}
        placeholders = {}
        counter = 0
        masked_text = text
        for pattern in self.patterns:
            def replace_func(match):
                nonlocal counter
                code_segment = match.group(1)
                key = f"__CODE_{counter}__"
                placeholders[key] = code_segment
                counter += 1
                return key
            masked_text = pattern.sub(replace_func, masked_text)
        return masked_text, placeholders
    def unmask(self, text, placeholders):
        if not text: return text
        result = text
        for key, val in placeholders.items():
            result = result.replace(key, val)
            if key not in result:
                result = re.sub(key.replace('_', r'\s*_\s*'), val, result)
        return result

class GlossaryManager:
    def __init__(self, global_csv, local_csv=None):
        self.term_map = {} 
        self.sorted_keys = []
        self.dropped_logs = []
        count_global = self.load_glossary(global_csv, "å…¨å±€")
        if local_csv and os.path.exists(local_csv):
            self.load_glossary(local_csv, "æœ¬åœ°")
        self.sorted_keys = sorted(self.term_map.keys(), key=lambda x: len(x), reverse=True)
        if self.dropped_logs:
            with open(DROPPED_LOG_PATH, 'w', encoding='utf-8') as f:
                f.write("\n".join(self.dropped_logs))
        print(f"æœ¯è¯­åº“åŠ è½½å®Œæ¯•: {len(self.sorted_keys)} æ¡æœ‰æ•ˆæœ¯è¯­")

    def load_glossary(self, path, label):
        if not os.path.exists(path): return 0
        encodings = ['utf-8', 'utf-8-sig', 'gb18030', 'gbk']
        df = None
        for enc in encodings:
            try:
                df = pd.read_csv(path, encoding=enc)
                break
            except: continue
        if df is None: return 0
        records = df.to_dict('records')
        count = 0
        for row in records:
            cn = str(row.get('Target', row.get('target', row.get('0', '')))).strip()
            en = str(row.get('Source', row.get('source', row.get('1', '')))).strip()
            if cn and en and en.lower() != 'nan':
                if en in self.term_map:
                    old_cn = self.term_map[en]['target']
                    if old_cn != cn:
                        self.dropped_logs.append(f"[{label}è¦†ç›–] '{en}': '{old_cn}' -> '{cn}'")
                flags = 0 if any(c.isupper() for c in en) else re.IGNORECASE
                self.term_map[en] = {
                    "target": cn, 
                    "source_original": en,
                    "regex": re.compile(r'\b' + re.escape(en) + r'\b', flags)
                }
                count += 1
        return count

    def pre_inject_text(self, text: str, json_path: str):
        if not text: return text, []
        injected_terms = [] 
        temp_text = text
        placeholders = {}
        placeholder_idx = 0
        text_lower = temp_text.lower()
        text_tokens = set(re.findall(r'[a-z]+', text_lower))
        candidates = []
        for k in self.sorted_keys:
            k_lower = k.lower()
            if " " not in k_lower:
                if k_lower in text_tokens: candidates.append(k)
            else:
                if k_lower in text_lower: candidates.append(k)
        for k in candidates:
            data = self.term_map[k]
            pattern = data["regex"]
            glossary_term = data["source_original"]
            matches = list(pattern.finditer(temp_text))
            if matches:
                def replace_func(match):
                    nonlocal placeholder_idx
                    matched_text = match.group(0)
                    should_inject = False
                    if matched_text == glossary_term: should_inject = True
                    elif glossary_term.islower() and matched_text.istitle(): should_inject = True
                    if should_inject:
                        injected_terms.append((glossary_term, data["target"]))
                        key = f"__TERM_{placeholder_idx}__"
                        injection_str = f"âŸª{data['target']}|åŸæ–‡:{matched_text}âŸ«"
                        placeholders[key] = injection_str
                        placeholder_idx += 1
                        return key
                    return matched_text
                temp_text = pattern.sub(replace_func, temp_text)
        final_text = temp_text
        for key, val in placeholders.items():
            final_text = final_text.replace(key, val)
        return final_text, injected_terms

# === æ–‡æœ¬å¤„ç†å·¥å…· ===

def cleanup_injection_tags(text):
    if not text: return ""
    return re.sub(r'âŸª(.*?)\|åŸæ–‡:.*?âŸ«', r'\1', text)

def clean_for_ai_audit(cn_text):
    if not cn_text: return ""
    if "<hr>" in cn_text: return cn_text.split("<hr>")[0].strip()
    if "<hr />" in cn_text: return cn_text.split("<hr />")[0].strip()
    if "åŸæ–‡:" in cn_text: return cn_text.split("åŸæ–‡:")[0].strip()
    return cn_text

def smart_format_bilingual(final_cn, original_en):
    if not final_cn: return original_en
    final_cn = cleanup_injection_tags(final_cn)
    cn = final_cn.strip().strip('"').strip("'")
    en = original_en.strip()
    en_clean = re.sub(r'[\s\W]', '', en).lower()
    cn_clean = re.sub(r'[\s\W]', '', cn).lower()
    if en_clean in cn_clean and len(en_clean) > 0: return cn 
    if "<p>" in en or "<br>" in en or len(en) > 80:
        return f"{cn}<br><br><hr><b>åŸæ–‡:</b><br>{en}"
    else:
        return f"{cn} {en}"

def strip_english_part(text, source_en):
    if not text: return ""
    if source_en and source_en in text:
        text = text.replace(source_en, "").strip()
    text = clean_for_ai_audit(text)
    match = re.search(r'[\u4e00-\u9fff]', text)
    if match:
        text = re.sub(r'\s*\(?[a-zA-Z0-9\s\-\']+\)?$', '', text).strip()
    return text

def extract_local_glossary(en_data, cn_data, output_path):
    print("æ­£åœ¨æ‰«ææœ¬åœ°æœ¯è¯­...")
    extracted = []
    def traverse(en_node, cn_node):
        if isinstance(en_node, dict) and isinstance(cn_node, dict):
            for k, v in en_node.items():
                if k in cn_node: traverse(v, cn_node[k])
        elif isinstance(en_node, list) and isinstance(cn_node, list):
            for i in range(min(len(en_node), len(cn_node))):
                traverse(en_node[i], cn_node[i])
        elif isinstance(en_node, str) and isinstance(cn_node, str):
            if len(en_node) < 60 and len(cn_node) > 0 and en_node != cn_node:
                clean_cn = strip_english_part(cn_node, en_node)
                if clean_cn and re.search(r'[\u4e00-\u9fff]', clean_cn):
                    extracted.append({'Source': en_node, 'Target': clean_cn})
    traverse(en_data, cn_data)
    while True:
        try:
            if extracted:
                df = pd.DataFrame(extracted).drop_duplicates(subset=['Source'])
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                print(f"å·²æå–æœ¬åœ°æœ¯è¯­: {len(df)} æ¡")
            break
        except PermissionError:
            print(f"\nâŒ é”™è¯¯ï¼šæ–‡ä»¶ '{output_path}' è¢«å ç”¨ã€‚è¯·å…³é—­ExcelåæŒ‰å›è½¦...")
            input()

def clean_response_text(text):
    if not text: return ""
    text = re.sub(r'^```[a-zA-Z]*\n', '', text)
    text = re.sub(r'\n```$', '', text)
    text = re.sub(r'^(Here is|Below is|ä»¥ä¸‹æ˜¯).*?(\n|$)', '', text, flags=re.IGNORECASE).strip()
    return text.strip()

protector = CodeProtector()

def process_single_item(task_type, en_text, cn_draft, glossary_mgr, path_str):
    if not en_text or len(en_text) < 2 or en_text.isdigit(): return en_text, None
    
    masked_text, code_placeholders = protector.mask(en_text)
    injected_text, injected_terms_list = glossary_mgr.pre_inject_text(masked_text, path_str)
    clean_draft = clean_for_ai_audit(cn_draft) if cn_draft else ""

    sys_header = "You are a professional Pathfinder 2e translator."
    sys_rules = "CRITICAL RULES:\n1. Output ONLY the translated Chinese text.\n2. Do NOT append the original English text at the end (I will handle it).\n3. Keep HTML tags/codes unchanged."
    tick_block = "```" 
    
    if task_type == "AUDIT":
        user_prompt = "Original:\n" + tick_block + f"\n{injected_text}\n" + tick_block + "\n\nExisting Draft:\n" + tick_block + f"\n{clean_draft}\n" + tick_block + "\n\nTask: Review the draft. If it is accurate, output it AS IS. If it is wrong, output a corrected Chinese translation."
    else:
        user_prompt = "Translate to Chinese:\n" + tick_block + f"\n{injected_text}\n" + tick_block

    for attempt in range(MAX_RETRIES):
        try:
            response = client.models.generate_content(
                model=MODEL_ID,
                contents=sys_header + "\n" + sys_rules + "\n" + user_prompt,
                config=types.GenerateContentConfig(temperature=0.1, safety_settings=SAFETY_SETTINGS)
            )
            
            if not response.text: raise ValueError("Empty response")
            trans = clean_response_text(response.text)
            
            final_trans = protector.unmask(trans, code_placeholders)
            final_trans = cleanup_injection_tags(final_trans)

            status = "New"
            if task_type == "AUDIT":
                clean_res = re.sub(r'\s', '', final_trans)
                clean_old = re.sub(r'\s', '', clean_draft)
                if clean_res == clean_old: 
                    status = "Kept"
                else: 
                    status = "Fixed"

            log_report(status, path_str, en_text, final_trans, injected_terms_list)
            
            return smart_format_bilingual(final_trans, en_text), status

        except Exception as e:
            wait_time = 2 * (attempt + 1) 
            if attempt == MAX_RETRIES - 1:
                write_process_log(f"Fail {path_str}: {e}")
                return smart_format_bilingual(cn_draft, en_text) if cn_draft else en_text, None
            else:
                time.sleep(wait_time) 

    return en_text, None

def log_report(status, path, original, translated, injected_terms):
    term_str = " | ".join([f"{e}->{c}" for e, c in injected_terms])
    row = {
        "JSON Path": path,
        "Involved Terms": term_str,
        "Original": original,
        "Translation": translated
    }
    with log_lock:
        if status in report_data:
            report_data[status].append(row)

def collect_tasks(en_data, cn_data, path_str="root"):
    tasks = []
    
    def get_cn_val(data, key):
        if isinstance(data, dict): return data.get(key)
        if isinstance(data, list) and isinstance(key, int) and key < len(data): return data[key]
        return None

    if isinstance(en_data, dict):
        iterator = en_data.items()
    elif isinstance(en_data, list):
        iterator = enumerate(en_data)
    else:
        return []

    for k, v in iterator:
        current_path = f"{path_str}.{k}" if isinstance(en_data, dict) else f"{path_str}[{k}]"
        
        cn_val = get_cn_val(cn_data, k)
        
        is_target_field = False
        if isinstance(en_data, dict) and k in TARGET_KEYS: is_target_field = True
        elif any(c in path_str.split('.') for c in SPECIAL_CONTAINERS): is_target_field = True

        if isinstance(v, str) and len(v) > 1 and is_target_field:
            if cn_val:
                content_hash = get_content_hash(v, cn_val)
                if content_hash in history_cache:
                    continue
            
            task_type = 'AUDIT' if (cn_val and isinstance(cn_val, str) and len(cn_val) > 0 and cn_val != v) else 'NEW'
            tasks.append({
                'type': task_type,
                'ref': en_data,
                'k': k,
                'en_v': v,
                'cn_v': cn_val if task_type == 'AUDIT' else None,
                'path': current_path
            })
        elif isinstance(v, (dict, list)):
            new_cn = cn_val if isinstance(cn_val, (dict, list)) else {}
            tasks.extend(collect_tasks(v, new_cn, current_path))
            
    return tasks

def save_logs():
    print("\næ­£åœ¨ç”Ÿæˆ Excel æŠ¥å‘Š...")
    try:
        with pd.ExcelWriter(REPORT_XLSX_PATH) as writer:
            pd.DataFrame(report_data["New"]).to_excel(writer, sheet_name="æ–°è¯‘(New)", index=False)
            pd.DataFrame(report_data["Fixed"]).to_excel(writer, sheet_name="ä¿®æ­£(Fixed)", index=False)
            pd.DataFrame(report_data["Kept"]).to_excel(writer, sheet_name="ä¿ç•™(Kept)", index=False)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {REPORT_XLSX_PATH}")
    except: pass

    with open(PROCESS_LOG_PATH, 'w', encoding='utf-8') as f:
        f.write("\n".join(process_log_buffer))
        
    save_history()

def main():
    print(f"PF2e æ±‰åŒ–è„šæœ¬ V26 (å…¨é‡å¿«ç…§å¤‡ä»½ç‰ˆ)")
    
    if not os.path.exists(SOURCE_EN_JSON_PATH):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°åŸºå‡†è‹±æ–‡æ–‡ä»¶ã€‚")
        return

    # 1. æ‰§è¡Œå…¨é‡å¤‡ä»½
    backup_existing_files()
    
    global history_cache
    history_cache = load_history()
    print(f"ğŸ§  å·²åŠ è½½å†å²ç¼“å­˜: {len(history_cache)} æ¡è®°å½•")

    print("è¯»å–æ–‡ä»¶...")
    with open(SOURCE_EN_JSON_PATH, 'r', encoding='utf-8-sig') as f:
        en_data = json.load(f)
    
    cn_data = {}
    if os.path.exists(TARGET_JSON_PATH):
        print(f"ğŸ”„ åŠ è½½ä¸Šæ¬¡æˆæœ: {TARGET_JSON_PATH}")
        try:
            with open(TARGET_JSON_PATH, 'r', encoding='utf-8-sig') as f:
                cn_data = json.load(f)
            extract_local_glossary(en_data, cn_data, LOCAL_GLOSSARY_EXPORT_PATH)
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç›®æ ‡æ–‡ä»¶å¤±è´¥ ({e})ï¼Œå°†æ‰§è¡Œå…¨é‡æ–°è¯‘")
    else:
        print("âœ¨ æ— å†å²æ–‡ä»¶ï¼Œå°†æ‰§è¡Œå…¨é‡æ–°è¯‘")
    
    glossary = GlossaryManager(GLOBAL_GLOSSARY_PATH, LOCAL_GLOSSARY_EXPORT_PATH)
    
    print("æ„å»ºä»»åŠ¡é˜Ÿåˆ— (è‡ªåŠ¨è·³è¿‡å·²éªŒè¯æ¡ç›®)...")
    all_tasks = collect_tasks(en_data, cn_data)
    print(f"å½“å‰å¾…å¤„ç†ä»»åŠ¡æ•°: {len(all_tasks)}")

    if not all_tasks:
        print("ğŸ‰ æ²¡æœ‰éœ€è¦æ›´æ–°çš„å†…å®¹ï¼æ‰€æœ‰æ¡ç›®å‡å·²é€šè¿‡éªŒè¯ã€‚")
        return

    rate_limiter = RateLimiter(TARGET_RPM)

    print("ğŸš€ å¼•æ“å¯åŠ¨...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_task = {}
        for t in tqdm(all_tasks, desc="åˆ†å‘ä»»åŠ¡"):
            rate_limiter.wait_for_slot()
            future = executor.submit(process_single_item, t['type'], t['en_v'], t['cn_v'], glossary, t['path'])
            future_to_task[future] = t
        
        print("\nâ³ ç­‰å¾…å›æ”¶ç»“æœ...")
        for future in tqdm(concurrent.futures.as_completed(future_to_task), total=len(all_tasks), desc="å›æ”¶ç»“æœ"):
            task = future_to_task[future]
            try:
                result_text, status = future.result()
                task['ref'][task['k']] = result_text
                
                if status == "Kept":
                    h = get_content_hash(task['en_v'], result_text)
                    with log_lock:
                        new_history_entries.add(h)
                        
            except Exception as e: pass

    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(en_data, f, ensure_ascii=False, indent=2)
    
    save_logs()
    print("ğŸ‰ å…¨éƒ¨å®Œæˆã€‚")

if __name__ == "__main__":
    main()