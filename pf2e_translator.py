import json
import os
import time
import re
import hashlib
import concurrent.futures
import pandas as pd
from pathlib import Path
from datetime import datetime
from threading import Lock
from tqdm import tqdm

# éœ€è¦å®‰è£…: pip install google-genai openai pandas openpyxl
from google import genai
from google.genai import types
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================

# API é…ç½® (ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1") 

# 1. [æ ¸å¿ƒå¼€å…³] åŒæ­¥æ¨¡å¼é€‰æ‹©
# "TARGET_MASTER": ä»¥ç›®æ ‡æ–‡ä»¶(CN)ç»“æ„ä¸ºå‡†ã€‚ä¸æ–°å¢Keyï¼Œåªç¿»è¯‘ç°æœ‰çš„ã€‚ç»“æ„ç»å¯¹å®‰å…¨ã€‚(è§£å†³è¯»ä¸å‡ºæ¥çš„é—®é¢˜)
# "SOURCE_MASTER": ä»¥æºæ–‡ä»¶(EN)ç»“æ„ä¸ºå‡†ã€‚ä¼šè‡ªåŠ¨è¡¥å…¨ç¼ºå¤±çš„Keyã€‚å¯èƒ½å¯¼è‡´ç»“æ„æ”¹å˜ã€‚
SYNC_MODE = "TARGET_MASTER" 

# 2. æ ¸å¿ƒæ–‡ä»¶é…ç½® (è‡ªåŠ¨è½¬æ¢ä¸ºPathå¯¹è±¡)
SOURCE_EN_JSON_PATH = Path("pf2e-beginner-box-en.json")  # ä»…ç”¨äºå‚è€ƒåŸæ–‡
TARGET_JSON_PATH = Path("pf2e-beginner-box.adventures.json")  # æ—¢æ˜¯ç»“æ„æ¨¡æ¿ï¼Œä¹Ÿæ˜¯è¾“å‡ºç›®æ ‡

# 3. æ¨¡å‹ä¼˜å…ˆçº§ (è‡ªåŠ¨é™çº§)
MODEL_PRIORITY_LIST = [
    ("openai", "gpt-5.2"),          # ä¼˜å…ˆçº§1
    ("openai", "gpt-5-mini"),       # ä¼˜å…ˆçº§2
    ("google", "gemini-3-flash-preview"), # ä¼˜å…ˆçº§3
]

# 4. æ€§èƒ½é…ç½®
MAX_WORKERS = 16    
TARGET_RPM = 450   
MAX_RETRIES = 5     
MAX_AUDIT_ROUNDS = 1  # æ ¡å¯¹æœ€å¤šè½®æ•°

# 5. [æ ¸å¿ƒå¼€å…³] æš´åŠ›é˜²æ¼æ¨¡å¼ (ä»…åœ¨ç¿»è¯‘å†…å®¹åˆ¤æ–­æ—¶ç”Ÿæ•ˆ)
BRUTE_FORCE_MODE = False

# 8. è¾“å‡ºé£æ ¼é…ç½®
# - FULL_BILINGUAL_MODE: å…¨å­—æ®µåŒè¯­ï¼ˆè‡ªåŠ¨è¡¥å…¨è‹±æ–‡ï¼‰
# - BILINGUAL_KEYS: è¾“å‡ºâ€œä¸­æ–‡ è‹±æ–‡â€
# - CN_ONLY_KEYS: ä»…è¾“å‡ºä¸­æ–‡ï¼ˆå½“ FULL_BILINGUAL_MODE=True æ—¶ä¹Ÿä¼šè¡¥è‹±æ–‡ï¼‰
# - LONG_TEXT_KEYS: é•¿æ–‡æœ¬ï¼ˆå½“ FULL_BILINGUAL_MODE=True æ—¶è¿½åŠ åŸæ–‡å—ï¼‰
FULL_BILINGUAL_MODE = True
BILINGUAL_KEYS = {"name", "label", "navName", "header", "tooltip"}
CN_ONLY_KEYS = {"tokenName", "caption"}
LONG_TEXT_KEYS = {"description", "text", "content", "gm_notes", "gm_description", "publicnotes", "publicNotes"}

# 9. å®¹å™¨ç¿»è¯‘ç­–ç•¥
# - TRANSLATE_MACROS: å®åç§°æ˜¯å¦ç¿»è¯‘
TRANSLATE_MACROS = True
SKIP_CONTAINERS = set()

# 6. æœ¯è¯­è¡¨ä¸æ—¥å¿— (è‡ªåŠ¨è½¬æ¢ä¸ºPathå¯¹è±¡)
GLOBAL_GLOSSARY_PATH = Path("æœ¯è¯­è¯‘åå¯¹ç…§è¡¨.csv") 
LOCAL_GLOSSARY_EXPORT_PATH = Path("æœ¯è¯­è¡¨_æœ¬åœ°æå–.csv")
REPORT_XLSX_PATH = Path("ç¿»è¯‘å®¡æŸ¥æŠ¥å‘Š.xlsx")
PROCESS_LOG_PATH = Path("è¿è¡Œæ—¥å¿—.txt")
MISSED_LOG_PATH = Path("å¤±è´¥æ¼ç¿»è®°å½•.txt")
HISTORY_FILE_PATH = Path("translation_history.json")
BACKUP_DIR = Path("backups")

# 7. æ—¥å¿—è¾“å‡º
PRINT_LOG_TO_TERMINAL = True  # åŒæ­¥è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¾¿äºå®æ—¶è§‚å¯Ÿ
USE_TQDM_WRITE = True         # ä½¿ç”¨ tqdm.write é¿å…æ‰“æ–­è¿›åº¦æ¡

# ç›®æ ‡å­—æ®µç™½åå•
TARGET_KEYS = {
    "name", "description", "text", "label", "caption", "value", 
    "unidentifiedName", "tokenName", "publicnotes", "publicNotes",
    "gm_notes", "gm_description", "header", "content", "items", 
    "navName", "tooltip", "preAuthored"
}
SPECIAL_CONTAINERS = {
    "notes", "folders", "journal", "journals", "scenes", 
    "actors", "items", "pages", "entries", "flags", "system"
}

# ===========================================

# åˆå§‹åŒ–å®¢æˆ·ç«¯
google_client = None
openai_client = None

if any(p == "google" for p, m in MODEL_PRIORITY_LIST) and GOOGLE_API_KEY:
    google_client = genai.Client(api_key=GOOGLE_API_KEY)

if any(p == "openai" for p, m in MODEL_PRIORITY_LIST) and OPENAI_API_KEY:
    openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

log_lock = Lock()
report_data = {"New": [], "Fixed": [], "Kept": [], "TermAdjusted": []}
process_log_buffer = []
missed_log_buffer = [] 
history_cache = set()
new_history_entries = set()

GOOGLE_SAFETY = [
    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
]

def write_process_log(msg):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—å†™å…¥"""
    with log_lock:
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        line = f"[{timestamp}] {msg}"
        process_log_buffer.append(line)
        if PRINT_LOG_TO_TERMINAL:
            if USE_TQDM_WRITE:
                tqdm.write(line)
            else:
                print(line)
        # æ¯100æ¡æ—¥å¿—è‡ªåŠ¨åˆ·ç›˜ä¸€æ¬¡
        if len(process_log_buffer) >= 100:
            _flush_process_log()

def _flush_process_log():
    """åˆ·æ–°è¿›ç¨‹æ—¥å¿—åˆ°æ–‡ä»¶"""
    if process_log_buffer and PROCESS_LOG_PATH:
        try:
            with PROCESS_LOG_PATH.open('a', encoding='utf-8') as f:
                f.write("\n".join(process_log_buffer) + "\n")
            process_log_buffer.clear()
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")

def write_missed_log(path, text, reason):
    """è®°å½•æ¼ç¿»æ¡ç›®"""
    with log_lock:
        missed_log_buffer.append(f"ã€{reason}ã€‘Path: {path}\nText: {text[:50]}...\n{'-'*30}")

# === AI è°ƒç”¨æ¥å£ ===
def call_ai_with_fallback(sys_prompt, user_prompt, path_str):
    """å¸¦å›é€€æœºåˆ¶çš„AIè°ƒç”¨
    
    ä¼˜å…ˆçº§é¡ºåºï¼š
    1. OpenAI GPT-5.2
    2. OpenAI GPT-5-mini  
    3. Google Gemini-3-flash-preview
    """
    last_error = None
    for provider, model_id in MODEL_PRIORITY_LIST:
        if provider == "google" and not google_client:
            continue
        if provider == "openai" and not openai_client:
            continue
        
        for attempt in range(MAX_RETRIES):
            try:
                write_process_log(f"ğŸ§  è°ƒç”¨æ¨¡å‹: {model_id} | ç¬¬{attempt+1}æ¬¡ | {path_str}")
                if provider == "google":
                    response = google_client.models.generate_content(
                        model=model_id,
                        contents=f"{sys_prompt}\n{user_prompt}",
                        config=types.GenerateContentConfig(temperature=0.1, safety_settings=GOOGLE_SAFETY)
                    )
                    if not response.text:
                        raise ValueError("Empty Google Response")
                    write_process_log(f"âœ… æ¨¡å‹å®Œæˆ: {model_id} | {path_str}")
                    return response.text
                elif provider == "openai":
                    if "gpt-5" in model_id or "o1" in model_id or "o3" in model_id:
                        # ä½¿ç”¨ Responses API
                        response = openai_client.responses.create(
                            model=model_id,
                            instructions=sys_prompt,
                            input=user_prompt,
                            reasoning={"effort": "none"}
                        )
                        write_process_log(f"âœ… æ¨¡å‹å®Œæˆ: {model_id} | {path_str}")
                        return response.output_text
                    else:
                        # ä½¿ç”¨ Chat Completions
                        response = openai_client.chat.completions.create(
                            model=model_id,
                            messages=[
                                {"role": "system", "content": sys_prompt},
                                {"role": "user", "content": user_prompt}
                            ],
                            temperature=0.1
                        )
                        write_process_log(f"âœ… æ¨¡å‹å®Œæˆ: {model_id} | {path_str}")
                        return response.choices[0].message.content
            except Exception as e:
                last_error = e
                write_process_log(f"âš ï¸ æ¨¡å‹å¤±è´¥: {model_id} | {path_str} | {e}")
                # é‡åˆ°é€Ÿç‡é™åˆ¶ç«‹å³é‡è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                if "429" in str(e) or "Resource Unavailable" in str(e):
                    break
                # å…¶ä»–é”™è¯¯ç­‰å¾…åé‡è¯•
                time.sleep(1 * (attempt + 1))
        
        write_process_log(f"âš ï¸ æ¨¡å‹ {model_id} å¤±è´¥: {last_error} -> å°è¯•ä¸‹ä¸€é¡ºä½")
    
    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    raise last_error

# === åŸºç¡€å·¥å…· ===
def backup_existing_files():
    """å¤‡ä»½ç°æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•"""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    files = [TARGET_JSON_PATH, REPORT_XLSX_PATH, LOCAL_GLOSSARY_EXPORT_PATH, HISTORY_FILE_PATH]
    for file_path in files:
        if file_path.exists():
            try:
                backup_path = BACKUP_DIR / f"{timestamp}_{file_path.name}.bak"
                backup_path.write_bytes(file_path.read_bytes())
                write_process_log(f"âœ… å¤‡ä»½å®Œæˆ: {file_path.name} -> {backup_path}")
            except Exception as e:
                write_process_log(f"å¤‡ä»½å¤±è´¥: {file_path.name} - {e}")

def get_content_hash(en, cn):
    return hashlib.md5(f"{en or ''}::{cn or ''}".encode('utf-8')).hexdigest()

def load_history():
    """ä»å†å²æ–‡ä»¶åŠ è½½å·²å¤„ç†é¡¹ç›®çš„å“ˆå¸Œå€¼"""
    if not HISTORY_FILE_PATH.exists():
        return set()
    try:
        with HISTORY_FILE_PATH.open('r', encoding='utf-8') as f:
            return set(json.load(f))
    except Exception as e:
        write_process_log(f"âš ï¸ åŠ è½½å†å²æ–‡ä»¶å¤±è´¥: {e}")
        return set()

def save_history():
    """å°†ç¼“å­˜å’Œæ–°æ¡ç›®ä¿å­˜åˆ°å†å²æ–‡ä»¶"""
    try:
        with HISTORY_FILE_PATH.open('w', encoding='utf-8') as f:
            json.dump(list(history_cache.union(new_history_entries)), f)
    except Exception as e:
        write_process_log(f"âš ï¸ ä¿å­˜å†å²æ–‡ä»¶å¤±è´¥: {e}")

class RateLimiter:
    def __init__(self, rpm):
        self.interval = 60.0 / rpm
        self.last_dispatch_time = 0
    def wait_for_slot(self):
        now = time.time()
        wait = self.last_dispatch_time + self.interval - now
        if wait > 0: time.sleep(wait)
        self.last_dispatch_time = time.time()

class CodeProtector:
    def __init__(self):
        self.patterns = [
            re.compile(r'(@[a-zA-Z0-9]+\[[^\]]*\])'),
            re.compile(r'(\[\[.*?\]\])'),
            re.compile(r'(<[^>]+>)'),
            re.compile(r'(&[a-zA-Z0-9#]+;)'),
        ]
    def mask(self, text):
        if not text: return text, {}
        ph, ctr = {}, 0
        def repl(m):
            nonlocal ctr
            k = f"__CODE_{ctr}__"
            ph[k] = m.group(1)
            ctr += 1
            return k
        for p in self.patterns: text = p.sub(repl, text)
        return text, ph
    def unmask(self, text, ph):
        if not text: return text
        for k, v in ph.items():
            text = text.replace(k, v)
            if k not in text: text = re.sub(k.replace('_', r'\s*_\s*'), v, text)
        return text

class GlossaryManager:
    """æœ¯è¯­è¡¨ç®¡ç†å™¨
    
    è´Ÿè´£åŠ è½½æœ¯è¯­è¡¨ã€åŒ¹é…å’Œæ³¨å…¥æœ¯è¯­åˆ°æ–‡æœ¬
    æ”¯æŒå¤šç§ç¼–ç çš„CSVæ–‡ä»¶
    """
    
    def __init__(self, global_csv, local_csv=None):
        """åˆå§‹åŒ–æœ¯è¯­è¡¨
        
        Args:
            global_csv: å…¨å±€æœ¯è¯­è¡¨è·¯å¾„
            local_csv: æœ¬åœ°æœ¯è¯­è¡¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.term_map = {}  # æœ¯è¯­æ˜ å°„ {è‹±æ–‡: {target, org, re}}
        self.sorted_keys = []
        self.load_glossary(global_csv)
        if local_csv:
            local_csv = Path(local_csv)
            if local_csv.exists():
                self.load_glossary(local_csv)
        # æŒ‰é•¿åº¦é™åºæ’åºï¼Œé˜²æ­¢çŸ­è¯å…ˆåŒ¹é…
        self.sorted_keys = sorted(self.term_map.keys(), key=lambda x: len(x), reverse=True)
        print(f"æœ¯è¯­åº“åŠ è½½: {len(self.sorted_keys)} æ¡")

    def load_glossary(self, path):
        """åŠ è½½CSVæœ¯è¯­è¡¨ï¼Œæ”¯æŒå¤šç§ç¼–ç """
        path = Path(path)
        if not path.exists():
            return
        df = None
        for enc in ['utf-8', 'utf-8-sig', 'gbk']:
            try:
                df = pd.read_csv(path, encoding=enc)
                break
            except Exception:
                continue
        if df is None:
            return
        for r in df.to_dict('records'):
            cn, en = str(r.get('Target', '')).strip(), str(r.get('Source', '')).strip()
            if cn and en and en.lower() != 'nan':
                flags = 0 if any(c.isupper() for c in en) else re.IGNORECASE
                self.term_map[en] = {"target": cn, "org": en, "re": re.compile(r'\b' + re.escape(en) + r'\b', flags)}

    def pre_inject_text(self, text, path_str):
        """åœ¨æ–‡æœ¬ä¸­æ³¨å…¥æœ¯è¯­æ ‡è®°ï¼Œä¾›AIè¯†åˆ«
        
        Args:
            text: å¾…å¤„ç†æ–‡æœ¬
            path_str: JSONè·¯å¾„ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Returns:
            (æ³¨å…¥åçš„æ–‡æœ¬, æ³¨å…¥çš„æœ¯è¯­åˆ—è¡¨)
        """
        if not text:
            return text, []
        
        inj, ph, idx = [], {}, 0
        # å¿«é€Ÿè¿‡æ»¤ï¼šåªæ£€æŸ¥æ–‡æœ¬ä¸­å®é™…åŒ…å«çš„è¯æ±‡
        tokens = set(re.findall(r'[a-z]+', text.lower()))
        cands = [k for k in self.sorted_keys if (k.lower() in tokens) or (" " in k and k.lower() in text.lower())]
        
        for k in cands:
            d = self.term_map[k]
            matches = list(d["re"].finditer(text))
            if matches:
                def repl(m):
                    nonlocal idx
                    mt = m.group(0)
                    # ä¿æŒåŸæœ‰çš„å¤§å°å†™æ ·å¼
                    if mt == d["org"] or (d["org"].islower() and mt.istitle()):
                        inj.append((d["org"], d["target"]))
                        k_ph = f"__Tm_{idx}__"
                        ph[k_ph] = f"âŸª{d['target']}|åŸæ–‡:{mt}âŸ«"
                        idx += 1
                        return k_ph
                    return mt
                text = d["re"].sub(repl, text)
        
        # å°†å ä½ç¬¦æ›¿æ¢ä¸ºæ ‡è®°
        for k, v in ph.items():
            text = text.replace(k, v)
        
        return text, inj

def smart_format_bilingual(cn, en):
    """å…¼å®¹æ—§é€»è¾‘çš„åŒè¯­æ ¼å¼åŒ–ï¼ˆä»…ç”¨äºæœ¯è¯­æå–/å…œåº•ï¼‰"""
    if not cn:
        return en
    if not en:
        return cn
    cn = re.sub(r'âŸª(.*?)\|åŸæ–‡:.*?âŸ«', r'\1', cn)
    return cn

def extract_last_key(path_str):
    """ä» JSON è·¯å¾„ä¸­æå–æœ€åä¸€ä¸ª keyï¼ˆå¿½ç•¥æ•°ç»„ä¸‹æ ‡ï¼‰"""
    if not path_str:
        return ""
    # å»æ‰æ•°ç»„ä¸‹æ ‡
    cleaned = re.sub(r"\[\d+\]", "", path_str)
    return cleaned.split(".")[-1] if "." in cleaned else cleaned

def strip_codes_for_lang_detect(text):
    if not text:
        return ""
    text = re.sub(r'@UUID\[[^\]]+\]', ' ', text)
    text = re.sub(r'@Compendium\[[^\]]+\]', ' ', text)
    text = re.sub(r'\[\[.*?\]\]', ' ', text)
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
    return text

def contains_english(text):
    return bool(re.search(r'[A-Za-z]', strip_codes_for_lang_detect(text)))

def contains_chinese(text):
    return bool(re.search(r'[\u4e00-\u9fff]', text or ''))

def get_value_style(path_str, key):
    path_segments = path_str.split('.')
    if any(seg in SKIP_CONTAINERS for seg in path_segments):
        return "skip"
    if "macros" in path_segments and not TRANSLATE_MACROS:
        return "skip"
    if "notes" in path_segments:
        return "cn_only"
    if "folders" in path_segments:
        return "bilingual"
    if key in CN_ONLY_KEYS:
        return "cn_only"
    if key in BILINGUAL_KEYS:
        return "bilingual"
    if key in LONG_TEXT_KEYS:
        return "cn_only"
    return "cn_only"

def strip_original_block(text):
    if not text:
        return text
    text = re.sub(r'<br><br><hr><b>(åŸæ–‡|Original):</b><br>.*$', '', text, flags=re.DOTALL)
    return text

def normalize_bilingual_short(cn_text, en_text):
    if not cn_text:
        return en_text or cn_text
    if not en_text:
        return cn_text
    clean_cn = strip_english_tokens(cn_text)
    clean_cn = collapse_duplicate_cn_prefix(clean_cn)
    clean_cn = collapse_duplicate_numeric_suffix(clean_cn)
    return f"{clean_cn} {en_text}" if clean_cn else en_text

def strip_trailing_english(text, min_len=30, min_words=4):
    if not text:
        return text
    prot = CodeProtector()
    masked, ph = prot.mask(text)
    m = re.search(r"(\s+[A-Za-z][A-Za-z0-9'â€™\-\s]*)$", masked)
    if m:
        tail = m.group(1)
        words = re.findall(r"[A-Za-z][A-Za-z0-9'â€™\-]*", tail)
        if len(words) >= min_words or len(tail) >= min_len:
            masked = masked[:m.start()].strip()
    return prot.unmask(masked, ph)

def normalize_output_text(cn_text, en_text, path_str):
    if not cn_text:
        return cn_text
    key = extract_last_key(path_str)
    style = get_value_style(path_str, key)
    if style == "skip":
        return cn_text

    cn_text = cleanup_injection_tags(cn_text)
    if style == "cn_only":
        cn_text = strip_original_block(cn_text)
        if FULL_BILINGUAL_MODE and en_text:
            return normalize_bilingual_short(cn_text, en_text).strip()
        if contains_chinese(cn_text) and contains_english(cn_text):
            cn_text = strip_trailing_english(cn_text)
        return cn_text.strip()

    cn_text = strip_original_block(cn_text)
    if en_text:
        return normalize_bilingual_short(cn_text, en_text).strip()
    return cn_text.strip()

def normalize_output_inplace(cn_node, en_node=None, path_str="root"):
    """å…¨é‡è§„èŒƒåŒ–è¾“å‡ºæ ¼å¼ï¼ˆä¸è§¦å‘AIï¼‰"""
    fixed = 0
    if isinstance(cn_node, dict):
        for k, v in cn_node.items():
            cur_path = f"{path_str}.{k}"
            en_v = None
            if isinstance(en_node, dict):
                en_v = en_node.get(k)
            if isinstance(v, str):
                new_v = normalize_output_text(v, en_v, cur_path)
                if new_v != v:
                    cn_node[k] = new_v
                    fixed += 1
            elif isinstance(v, (dict, list)):
                fixed += normalize_output_inplace(v, en_v, cur_path)
    elif isinstance(cn_node, list):
        for i, v in enumerate(cn_node):
            cur_path = f"{path_str}[{i}]"
            en_v = None
            if isinstance(en_node, list) and i < len(en_node):
                en_v = en_node[i]
            if isinstance(v, str):
                new_v = normalize_output_text(v, en_v, cur_path)
                if new_v != v:
                    cn_node[i] = new_v
                    fixed += 1
            elif isinstance(v, (dict, list)):
                fixed += normalize_output_inplace(v, en_v, cur_path)
    return fixed

def extract_local_glossary(en_data, cn_data, output_path):
    """ä»ç¿»è¯‘æ•°æ®ä¸­æå–æœ¬åœ°æœ¯è¯­è¡¨"""
    print("æ­£åœ¨æ‰«ææœ¬åœ°æœ¯è¯­...")
    write_process_log("å¼€å§‹æå–æœ¬åœ°æœ¯è¯­è¡¨")
    extracted = []
    
    def traverse(en_node, cn_node):
        """é€’å½’éå†æ•°æ®ç»“æ„ï¼Œæå–å·²ç¿»è¯‘é¡¹"""
        if isinstance(en_node, dict) and isinstance(cn_node, dict):
            for k, v in en_node.items():
                if k in cn_node:
                    traverse(v, cn_node[k])
        elif isinstance(en_node, list) and isinstance(cn_node, list):
            for i in range(min(len(en_node), len(cn_node))):
                traverse(en_node[i], cn_node[i])
        elif isinstance(en_node, str) and isinstance(cn_node, str):
            if len(en_node) < 60 and len(cn_node) > 0 and en_node != cn_node:
                clean_cn = smart_format_bilingual(cn_node, "")
                clean_cn = strip_english_tokens(clean_cn)
                clean_cn = collapse_duplicate_numeric_suffix(clean_cn)
                if clean_cn and re.search(r'[\u4e00-\u9fff]', clean_cn):
                    extracted.append({'Source': en_node, 'Target': clean_cn})
    
    traverse(en_data, cn_data)
    if extracted:
        try:
            df = pd.DataFrame(extracted).drop_duplicates(subset=['Source'])
            output_path = Path(output_path)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            write_process_log(f"æœ¬åœ°æœ¯è¯­è¡¨å¯¼å‡ºå®Œæˆ: {output_path} | æ¡ç›®: {len(df)}")
        except Exception as e:
            write_process_log(f"âš ï¸ å¯¼å‡ºæœ¯è¯­è¡¨å¤±è´¥: {e}")
    else:
        write_process_log("æœ¬åœ°æœ¯è¯­è¡¨ä¸ºç©ºï¼šæœªå¯¼å‡º")

def clean_response_text(text):
    """æ¸…ç†AIå“åº”æ–‡æœ¬"""
    if not text:
        return ""
    text = re.sub(r'^```[a-zA-Z]*\n', '', text)
    text = re.sub(r'\n```$', '', text)
    text = re.sub(r'^(Here is|Below is|ä»¥ä¸‹æ˜¯).*?(\n|$)', '', text, flags=re.IGNORECASE).strip()
    # ç§»é™¤æ¨¡å‹è¾“å‡ºçš„æ³¨é‡Šè¡Œï¼ˆä¾‹å¦‚ // ...ï¼‰
    text = re.sub(r'^\s*//.*$', '', text, flags=re.MULTILINE)
    return text.strip()

def cleanup_injection_tags(text):
    """æ¸…ç†æœ¯è¯­æ³¨å…¥æ ‡ç­¾"""
    if not text:
        return text
    # ç§»é™¤æ³¨å…¥æ ‡ç­¾ï¼Œä¿ç•™ç¿»è¯‘
    text = re.sub(r'âŸª(.*?)\|åŸæ–‡:.*?âŸ«', r'\1', text)
    # æ¸…ç†æ®‹ç•™çš„â€œ|åŸæ–‡:â€ç¢ç‰‡ï¼ˆå«ç¼ºå¤±å·¦æ‹¬å·çš„æƒ…å†µï¼‰
    text = re.sub(r'\s*\d+\|åŸæ–‡:[^âŸ«]*', '', text)
    text = re.sub(r'\s*\|åŸæ–‡:[^âŸ«]*', '', text)
    # æ¸…ç†æ®‹ç•™æ³¨å…¥ç¬¦å·ä¸ä»£ç å—æ ‡è®°
    text = text.replace('âŸª', '').replace('âŸ«', '')
    text = text.replace('```', '')
    return text

def collapse_duplicate_cn_prefix(text):
    """æ¸…ç†çŸ­æ–‡æœ¬å¼€å¤´çš„é‡å¤ä¸­æ–‡è¯ç»„

    ä¾‹: "å±æ€§å€¼ å±æ€§å€¼ Ability Scores" -> "å±æ€§å€¼ Ability Scores"
    """
    if not text:
        return text
    parts = text.split()
    if len(parts) < 2:
        return text
    if parts[0] == parts[1] and re.search(r'[\u4e00-\u9fff]', parts[0]):
        i = 1
        while i < len(parts) and parts[i] == parts[0]:
            i += 1
        return " ".join([parts[0]] + parts[i:])
    return text

def collapse_duplicate_numeric_suffix(text):
    """æ¸…ç†æœ«å°¾é‡å¤æ•°å­—ï¼ˆå¦‚: 01 01 / 2 2ï¼‰"""
    if not text:
        return text
    parts = text.split()
    while len(parts) >= 2 and parts[-1] == parts[-2] and re.fullmatch(r"\d+", parts[-1]):
        parts.pop()
    return " ".join(parts)

def strip_english_tokens(text):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„è‹±æ–‡è¯ï¼Œä¿ç•™ä¸­æ–‡ä¸æ•°å­—

    ç”¨äºæœ¬åœ°æœ¯è¯­æå–ï¼Œé¿å…æœ¯è¯­è¡¨æºå¸¦è‹±æ–‡å°¾å·´ã€‚
    """
    if not text:
        return text
    cleaned = re.sub(r"[A-Za-z][A-Za-z0-9'\-]*", "", text)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return cleaned if cleaned else text

def normalize_for_compare(text):
    return re.sub(r'\s', '', text or '')

def detect_adjusted_terms(output_text, injected_terms):
    """æ£€æµ‹æœ¯è¯­æ˜¯å¦è¢«æ”¹åŠ¨ï¼ˆè¾“å‡ºä¸­ç¼ºå¤±æœ¯è¯­è¯‘åï¼‰"""
    adjusted = []
    if not output_text or not injected_terms:
        return adjusted
    for en, target in injected_terms:
        if target and target not in output_text:
            adjusted.append((en, target))
    return adjusted

protector = CodeProtector()

def process_single_item(task_type, en_text, cn_draft, glossary_mgr, path_str, audit_mode=None):
    """å¤„ç†å•ä¸ªç¿»è¯‘é¡¹
    
    Args:
        task_type: 'NEW' æˆ– 'AUDIT'
        en_text: è‹±æ–‡åŸæ–‡
        cn_draft: ä¸­æ–‡åˆç¨¿ï¼ˆå®¡æ ¸æ¨¡å¼ï¼‰
        glossary_mgr: æœ¯è¯­è¡¨ç®¡ç†å™¨
        path_str: JSONè·¯å¾„
    
    Returns:
        (ç¿»è¯‘ç»“æœ, çŠ¶æ€)
    """
    # åŸºç¡€æ£€æŸ¥
    if not en_text or len(en_text) < 2:
        return en_text, None
    if not re.search(r'[a-zA-Z]', en_text):
        return en_text, None

    write_process_log(f"ğŸ§© å¤„ç†ä»»åŠ¡: {task_type} | {path_str}")
    
    # ä»£ç ä¿æŠ¤å’Œæœ¯è¯­æ³¨å…¥
    prot = CodeProtector()
    masked, code_ph = prot.mask(en_text)
    injected, terms = glossary_mgr.pre_inject_text(masked, path_str)
    
    # æ„å»ºå®¡æ ¡åˆç¨¿
    clean_draft_txt = cn_draft or ""
    
    # æ„å»ºæç¤ºè¯
    sys_prompt = (
        "You are a professional Pathfinder 2e translator. "
        "Output ONLY Simplified Chinese. Keep HTML/Foundry codes unchanged. "
        "Do NOT output notes, tags like âŸªâŸ« or |åŸæ–‡:, "
        "do NOT add markdown fences or comments, and do NOT repeat words. "
        "Keep numbers and units exactly as in the source."
    )
    audit_prompt = (
        "You are a professional Pathfinder 2e editor. "
        "Keep the Draft format unchanged. If the Draft contains English (e.g., after <hr>åŸæ–‡), "
        "preserve the English segment as-is. Only fix Chinese wording/grammar. "
        "Do NOT add/remove HTML or Foundry codes. Output the corrected Draft only."
    )
    if task_type == "AUDIT":
        prompt = (
            f"Original:\n```\n{injected}\n```\nDraft:\n```\n{clean_draft_txt}\n```\n"
            f"Task: Review draft. If correct, output it. If wrong, correct it."
        )
    else:
        prompt = f"Translate:\n```\n{injected}\n```"

    try:
        # è¯‘æ–‡/æ ¡å¯¹
        if task_type == "AUDIT":
            draft = clean_draft_txt
            changed = False
            final_trans = draft
            for _ in range(MAX_AUDIT_ROUNDS):
                res_text = call_ai_with_fallback(audit_prompt, (
                    f"Original:\n```\n{injected}\n```\n"
                    f"Draft:\n```\n{draft}\n```\n"
                    f"Task: Review draft. If correct, output it. If wrong, correct it."
                ), path_str)
                trans = clean_response_text(res_text)
                final_trans = prot.unmask(trans, code_ph)
                final_trans = cleanup_injection_tags(final_trans)
                final_trans = collapse_duplicate_cn_prefix(final_trans)
                final_trans = collapse_duplicate_numeric_suffix(final_trans)
                final_trans = normalize_output_text(final_trans, en_text, path_str)

                if normalize_for_compare(final_trans) == normalize_for_compare(draft):
                    break
                changed = True
                draft = final_trans

            status = "Fixed" if changed else "Kept"
            adjusted_terms = detect_adjusted_terms(final_trans, terms)
            log_report(status, path_str, en_text, final_trans, terms, adjusted_terms)
            write_process_log(f"âœ… ä»»åŠ¡å®Œæˆ: {status} | {path_str}")
            return final_trans, status

        # NEW ç¿»è¯‘
        res_text = call_ai_with_fallback(sys_prompt, prompt, path_str)
        trans = clean_response_text(res_text)
        final_trans = prot.unmask(trans, code_ph)
        final_trans = cleanup_injection_tags(final_trans)
        final_trans = collapse_duplicate_cn_prefix(final_trans)
        final_trans = collapse_duplicate_numeric_suffix(final_trans)
        final_trans = normalize_output_text(final_trans, en_text, path_str)

        adjusted_terms = detect_adjusted_terms(final_trans, terms)
        log_report("New", path_str, en_text, final_trans, terms, adjusted_terms)
        write_process_log(f"âœ… ä»»åŠ¡å®Œæˆ: New | {path_str}")
        return final_trans, "New"

    except Exception as e:
        write_process_log(f"âŒ æ‰€æœ‰æ¨¡å‹å¤±è´¥ {path_str}: {e}")
        write_missed_log(path_str, en_text, "All Models Failed")
        fallback = cn_draft if cn_draft else f"ã€FAILã€‘{en_text}"
        return fallback, None

def log_report(status, path, original, translated, injected_terms, adjusted_terms=None):
    term_str = " | ".join([f"{e}->{c}" for e, c in injected_terms])
    adjusted_str = " | ".join([f"{e}->{c}" for e, c in (adjusted_terms or [])])
    row = {
        "JSON Path": path,
        "Involved Terms": term_str,
        "Adjusted Terms": adjusted_str,
        "Original": original,
        "Translation": translated
    }
    with log_lock:
        if status in report_data:
            report_data[status].append(row)
        if adjusted_terms:
            report_data["TermAdjusted"].append(row)

# === V32 æ ¸å¿ƒï¼šä»»åŠ¡æ”¶é›†åˆ†æµ ===

def collect_tasks_source_master(en_data, cn_data, path_str="root"):
    """
    [æ—§é€»è¾‘] ä»¥ Source (è‹±æ–‡) ä¸ºä¸»ã€‚
    å¦‚æœ Source æœ‰ä½† Target æ²¡æœ‰ï¼Œä¼šæ–°å¢ï¼ˆå¯èƒ½ç ´å Target ç»“æ„ï¼‰ã€‚
    """
    tasks = []
    
    def get_cn(d, k):
        if isinstance(d, dict): return d.get(k)
        if isinstance(d, list) and isinstance(k, int) and k < len(d): return d[k]
        return None

    if isinstance(en_data, dict): iter_items = en_data.items()
    elif isinstance(en_data, list): iter_items = enumerate(en_data)
    else: return []

    for k, v in iter_items:
        cur_path = f"{path_str}.{k}" if isinstance(en_data, dict) else f"{path_str}[{k}]"
        cn_v = get_cn(cn_data, k)
        
        # åˆ¤æ–­é€»è¾‘
        should_translate = False
        if isinstance(v, str) and len(v) > 1:
            has_en = contains_english(v)
            has_cn = contains_chinese(v)
            is_file = v.lower().endswith(('.png', '.webp', '.jpg', '.mp3', '.ogg', '.m4a', '.webm'))
            is_target_key = isinstance(en_data, dict) and k in TARGET_KEYS
            is_in_container = any(c in path_str.split('.') for c in SPECIAL_CONTAINERS)
            if has_en and not has_cn and not is_file:
                if BRUTE_FORCE_MODE:
                    should_translate = True
                else:
                    if is_target_key or is_in_container:
                        should_translate = True

        if should_translate:
            if cn_v and get_content_hash(v, cn_v) in history_cache: continue
            tt = 'AUDIT' if (cn_v and isinstance(cn_v, str) and len(cn_v) > 0 and cn_v != v) else 'NEW'
            mode = None
            if tt == 'AUDIT' and isinstance(cn_v, str):
                has_en = bool(re.search(r'[a-zA-Z]', cn_v))
                mode = 'AUDIT_BILINGUAL' if has_en else 'AUDIT_CN_APPEND'
            tasks.append({'type': tt, 'mode': mode, 'ref': en_data, 'k': k, 'en_v': v, 'cn_v': cn_v if tt=='AUDIT' else None, 'path': cur_path})
            
        elif isinstance(v, (dict, list)):
            new_cn = cn_v if isinstance(cn_v, (dict, list)) else {}
            tasks.extend(collect_tasks_source_master(v, new_cn, cur_path))
            
    return tasks

def collect_tasks_target_master(cn_data, en_data, path_str="root"):
    """
    [V32 æ–°é€»è¾‘] ä»¥ Target (ä¸­æ–‡) ä¸ºä¸»ã€‚
    åªéå† Target çš„ç»“æ„ã€‚å¦‚æœ Target é‡Œæœ‰è‹±æ–‡ï¼Œå°±ç¿»è¯‘ã€‚
    å®Œå…¨å¿½ç•¥ Source ä¸­å¤šå‡ºæ¥çš„ç»“æ„ï¼ˆDumpæ‰ï¼‰ã€‚
    """
    tasks = []
    
    # è¾…åŠ©å‡½æ•°ï¼šå°è¯•åœ¨ Source æ•°æ®é‡Œæ‰¾åˆ°å¯¹åº”çš„è·¯å¾„ï¼Œä»¥è·å–æœ€çº¯æ­£çš„åŸæ–‡ï¼ˆç”¨äºå‚è€ƒï¼‰
    def get_en_counterpart(source_node, key):
        if source_node is None: return None
        if isinstance(source_node, dict): return source_node.get(key)
        if isinstance(source_node, list) and isinstance(key, int) and key < len(source_node): return source_node[key]
        return None

    if isinstance(cn_data, dict): iter_items = cn_data.items()
    elif isinstance(cn_data, list): iter_items = enumerate(cn_data)
    else: return []

    for k, v in iter_items:
        cur_path = f"{path_str}.{k}" if isinstance(cn_data, dict) else f"{path_str}[{k}]"
        
        # å°è¯•å» Source é‡Œæ‰¾å¯¹åº”çš„åŸæ–‡
        # å¦‚æœç»“æ„ä¸åŒ¹é…ï¼ˆTargetç»“æ„æµ…ï¼ŒSourceç»“æ„æ·±ï¼‰ï¼Œen_v å¯èƒ½æ˜¯ None
        en_v = get_en_counterpart(en_data, k)
        
        # åˆ¤æ–­é€»è¾‘
        should_translate = False
        mode = None
        if isinstance(v, str) and len(v) > 1:
            style = get_value_style(cur_path, k)
            if style != "skip":
                has_en = contains_english(v)
                has_cn = contains_chinese(v)
                is_file = v.lower().endswith(('.png', '.webp', '.jpg', '.mp3', '.ogg', '.m4a', '.webm'))
                if has_en and not has_cn and not is_file:
                    should_translate = True

        if should_translate:
            # å¦‚æœ Source é‡Œæ‰¾ä¸åˆ°å¯¹åº”çš„ en_v (å› ä¸ºç»“æ„ä¸åŒ)ï¼Œæˆ‘ä»¬å°±æŠŠå½“å‰ Target é‡Œçš„ v å½“ä½œåŸæ–‡
            original_text = en_v if (en_v and isinstance(en_v, str)) else v
            
            # æ£€æŸ¥ç¼“å­˜
            if get_content_hash(original_text, v) in history_cache: continue
            
            task_type = 'NEW'
            
            # æ³¨æ„ï¼šè¿™é‡Œçš„ ref æ˜¯ cn_dataï¼Œå› ä¸ºæˆ‘ä»¬è¦å›å†™åˆ° Target
            tasks.append({
                'type': task_type,
                'mode': mode,
                'ref': cn_data, 
                'k': k,
                'en_v': original_text, # é€ç»™ AI çš„å‚è€ƒåŸæ–‡
                'cn_v': v,             # é€ç»™ AI çš„ç°æœ‰è¯‘æ–‡ï¼ˆç”¨äºæ ¡å¯¹ï¼‰
                'path': cur_path
            })
            
        elif isinstance(v, (dict, list)):
            # é€’å½’æ—¶ï¼Œen_v å¯èƒ½æ˜¯ None (å¦‚æœç»“æ„å¯¹ä¸ä¸Š)ï¼Œè¿™æ²¡å…³ç³»ï¼Œç»§ç»­å¾€ä¸‹ä¼  None å³å¯
            tasks.extend(collect_tasks_target_master(v, en_v, cur_path))
            
    return tasks

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒå’ŒAPIé…ç½®"""
    issues = []
    
    # æ£€æŸ¥APIå¯†é’¥
    if not GOOGLE_API_KEY and not OPENAI_API_KEY:
        issues.append("âŒ æœªé…ç½®ä»»ä½•APIå¯†é’¥ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡: GOOGLE_API_KEY æˆ– OPENAI_API_KEY")
    
    if GOOGLE_API_KEY and not google_client:
        issues.append("âš ï¸ Google APIå¯†é’¥é…ç½®å¤±è´¥ï¼Œè·³è¿‡GoogleæœåŠ¡")
    
    if OPENAI_API_KEY and not openai_client:
        issues.append("âš ï¸ OpenAI APIå¯†é’¥é…ç½®å¤±è´¥ï¼Œè·³è¿‡OpenAIæœåŠ¡")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not SOURCE_EN_JSON_PATH.exists():
        issues.append(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {SOURCE_EN_JSON_PATH}")
    
    if SYNC_MODE == "TARGET_MASTER" and not TARGET_JSON_PATH.exists():
        issues.append(f"âŒ TARGET_MASTERæ¨¡å¼éœ€è¦ç›®æ ‡æ–‡ä»¶: {TARGET_JSON_PATH}")
    
    # è¾“å‡ºæ£€æŸ¥ç»“æœ
    for issue in issues:
        print(issue)
    
    # å¦‚æœæœ‰å…³é”®é”™è¯¯åˆ™è¿”å›False
    return len([i for i in issues if i.startswith("âŒ")]) == 0

def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•´ä¸ªç¿»è¯‘æµç¨‹"""
    print(f"PF2e æ±‰åŒ–è„šæœ¬ V32 (åŒæ­¥æ¨¡å¼: {SYNC_MODE})")
    print(f"æºæ–‡ä»¶: {SOURCE_EN_JSON_PATH.name if SOURCE_EN_JSON_PATH.exists() else '(ä¸å­˜åœ¨)'}")
    print(f"ç›®æ ‡æ–‡ä»¶: {TARGET_JSON_PATH.name if TARGET_JSON_PATH.exists() else '(ä¸å­˜åœ¨)'}")
    print("")
    
    # ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        return

    # å¤‡ä»½ç°æœ‰æ–‡ä»¶
    backup_existing_files()
    global history_cache
    history_cache = load_history()
    print(f"ğŸ§  å·²åŠ è½½ç¼“å­˜: {len(history_cache)} æ¡è®°å½•")
    write_process_log(f"ç¼“å­˜æ¡ç›®: {len(history_cache)}")

    # åŠ è½½æºæ–‡ä»¶
    with SOURCE_EN_JSON_PATH.open('r', encoding='utf-8-sig') as f:
        en_data = json.load(f)
    
    # åŠ è½½æˆ–åˆå§‹åŒ–ç›®æ ‡æ–‡ä»¶
    cn_data = {}
    if TARGET_JSON_PATH.exists():
        try:
            print("ğŸ”„ åŠ è½½ç›®æ ‡æ–‡ä»¶...")
            with TARGET_JSON_PATH.open('r', encoding='utf-8-sig') as f:
                cn_data = json.load(f)
            write_process_log(f"ç›®æ ‡æ–‡ä»¶åŠ è½½æˆåŠŸ: {TARGET_JSON_PATH}")
        except Exception as e:
            print(f"âŒ åŠ è½½ç›®æ ‡æ–‡ä»¶å¤±è´¥: {e}")
            if SYNC_MODE == "TARGET_MASTER":
                print("â›” åœ¨ TARGET_MASTER æ¨¡å¼ä¸‹ï¼Œç›®æ ‡æ–‡ä»¶å¿…é¡»å­˜åœ¨ä¸”æœ‰æ•ˆï¼")
                return
    else:
        if SYNC_MODE == "TARGET_MASTER":
            print("â›” é”™è¯¯ï¼šTARGET_MASTER æ¨¡å¼éœ€è¦ç›®æ ‡æ–‡ä»¶å­˜åœ¨ (ä½œä¸ºç»“æ„æ¨¡æ¿)ã€‚")
            return
        # Source Master æ¨¡å¼å¯ä»¥ä»ç©ºå¼€å§‹
        cn_data = {}

    extract_local_glossary(en_data, cn_data, LOCAL_GLOSSARY_EXPORT_PATH)
    glossary = GlossaryManager(GLOBAL_GLOSSARY_PATH, LOCAL_GLOSSARY_EXPORT_PATH)
    write_process_log(f"æœ¯è¯­åº“åŠ è½½å®Œæˆ: {len(glossary.sorted_keys)} æ¡")
    
    print("æ„å»ºä»»åŠ¡é˜Ÿåˆ—...")
    
    # === V32 åˆ†æµé€»è¾‘ ===
    if SYNC_MODE == "TARGET_MASTER":
        # éå† Targetï¼Œå¿½ç•¥ Source å¤šä½™ç»“æ„
        all_tasks = collect_tasks_target_master(cn_data, en_data)
    else:
        # éå† Sourceï¼Œå¼ºåˆ¶è¡¥å…¨ Target
        all_tasks = collect_tasks_source_master(en_data, cn_data)
        
    print(f"å¾…å¤„ç†ä»»åŠ¡: {len(all_tasks)}")
    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹
    type_counts = {"NEW": 0, "AUDIT": 0}
    for t in all_tasks:
        if t["type"] in type_counts:
            type_counts[t["type"]] += 1
    write_process_log(f"ä»»åŠ¡ç»Ÿè®¡: NEW={type_counts['NEW']}, AUDIT={type_counts['AUDIT']}")
    
    if not all_tasks: 
        print("ğŸ‰ æ²¡æœ‰éœ€è¦æ›´æ–°çš„å†…å®¹ï¼")
        return

    rl = RateLimiter(TARGET_RPM)
    print("ğŸš€ å¼•æ“å¯åŠ¨...")
    write_process_log(f"çº¿ç¨‹æ± å¯åŠ¨: workers={MAX_WORKERS}, RPM={TARGET_RPM}")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ä»»åŠ¡
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
        fut_map = {}
        # æäº¤ä»»åŠ¡
        for t in tqdm(all_tasks, desc="åˆ†å‘", position=0, leave=True, dynamic_ncols=True):
            rl.wait_for_slot()
            future = exe.submit(process_single_item, t['type'], t['en_v'], t['cn_v'], glossary, t['path'], t.get('mode'))
            fut_map[future] = t
        
        # æ”¶é›†ç»“æœ
        for f in tqdm(concurrent.futures.as_completed(fut_map), total=len(all_tasks), desc="å›æ”¶", position=1, leave=True, dynamic_ncols=True):
            task = fut_map[f]
            try:
                res, st = f.result(timeout=30)  # å•ä¸ªä»»åŠ¡è¶…æ—¶30ç§’
                task['ref'][task['k']] = res
                # å¦‚æœç¿»è¯‘æœªæ”¹å˜ï¼ŒåŠ å…¥ç¼“å­˜ä»¥åŠ å¿«ä¸‹æ¬¡è¿è¡Œ
                if st == "Kept":
                    with log_lock:
                        new_history_entries.add(get_content_hash(task['en_v'], res))
            except concurrent.futures.TimeoutError:
                write_process_log(f"âŒ ä»»åŠ¡è¶…æ—¶: {task['path']}")
            except Exception as e:
                write_process_log(f"âŒ ä»»åŠ¡å¤±è´¥: {task['path']} - {e}")

    print("æ­£åœ¨ä¿å­˜æœ€ç»ˆç»“æœ...")
    # Target Master æ¨¡å¼ï¼šä¿æŒTargetç»“æ„ï¼Œåªæ›´æ–°å€¼
    # Source Master æ¨¡å¼ï¼šä½¿ç”¨Sourceç»“æ„ï¼Œå¼ºåˆ¶è¡¥å…¨Target
    output_obj = cn_data if SYNC_MODE == "TARGET_MASTER" else en_data

    # è§„èŒƒåŒ–è¾“å‡ºé£æ ¼ï¼ˆçŸ­å­—æ®µåŒè¯­/ä¸­æ–‡ã€å»é™¤åŸæ–‡å—ï¼‰
    if SYNC_MODE == "TARGET_MASTER":
        fixed_cnt = normalize_output_inplace(cn_data, en_data)
        if fixed_cnt:
            write_process_log(f"ğŸ§¹ è¾“å‡ºè§„èŒƒåŒ–: {fixed_cnt} é¡¹")
    
    # ä¿å­˜ç¿»è¯‘ç»“æœ
    with TARGET_JSON_PATH.open('w', encoding='utf-8') as f:
        json.dump(output_obj, f, ensure_ascii=False, indent=2)
    write_process_log(f"å†™å…¥ç›®æ ‡æ–‡ä»¶: {TARGET_JSON_PATH}")
    
    # ä¿å­˜é—æ¼æ—¥å¿—
    if missed_log_buffer:
        with MISSED_LOG_PATH.open('w', encoding='utf-8') as f:
            f.write("\n".join(missed_log_buffer))
        print(f"âš ï¸ è­¦å‘Šï¼šæœ‰ {len(missed_log_buffer)} æ¡å†…å®¹æ¼ç¿»")
    
    # ä¿å­˜å®¡æŸ¥æŠ¥å‘Šï¼ˆæ”¯æŒé‡è¯•ï¼‰
    max_retry = 3
    for attempt in range(max_retry):
        try:
            with pd.ExcelWriter(REPORT_XLSX_PATH) as w:
                pd.DataFrame(report_data["New"]).to_excel(w, sheet_name="New", index=False)
                pd.DataFrame(report_data["Fixed"]).to_excel(w, sheet_name="Fixed", index=False)
                pd.DataFrame(report_data["Kept"]).to_excel(w, sheet_name="Kept", index=False)
                pd.DataFrame(report_data["TermAdjusted"]).to_excel(w, sheet_name="TermAdjusted", index=False)
            break
        except PermissionError:
            if attempt < max_retry - 1:
                input(f"âŒ è¯·å…³é—­ {REPORT_XLSX_PATH} åå›è½¦...")
            else:
                print(f"âš ï¸ æ— æ³•ä¿å­˜æŠ¥å‘Šï¼Œæ–‡ä»¶è¢«å ç”¨")
        except Exception as e:
            write_process_log(f"âš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            break

    save_history()
    write_process_log("å†å²ç¼“å­˜å·²ä¿å­˜")
    # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒº
    _flush_process_log()
    write_process_log("æ—¥å¿—å·²åˆ·æ–°")
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“Š æœ¬æ¬¡è¿è¡Œç»Ÿè®¡:")
    print(f"   æ–°å¢ç¿»è¯‘: {len(report_data['New'])} é¡¹")
    print(f"   ä¿®å¤ç¿»è¯‘: {len(report_data['Fixed'])} é¡¹")
    print(f"   ä¿æŒä¸å˜: {len(report_data['Kept'])} é¡¹")
    if missed_log_buffer:
        print(f"   âš ï¸ æ¼ç¿»é¡¹ç›®: {len(missed_log_buffer)} é¡¹")
    print("="*50)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆã€‚")

if __name__ == "__main__":
    main()