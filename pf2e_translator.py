import json
import os
import time
import re
import hashlib
import concurrent.futures
import pandas as pd
from pathlib import Path
from datetime import datetime
from threading import Lock
from tqdm import tqdm

# éœ€è¦å®‰è£…: pip install google-genai openai pandas openpyxl
from google import genai
from google.genai import types
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================

# API é…ç½® (ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1") 

# 1. [æ ¸å¿ƒå¼€å…³] åŒæ­¥æ¨¡å¼é€‰æ‹©
# "TARGET_MASTER": ä»¥ç›®æ ‡æ–‡ä»¶(CN)ç»“æ„ä¸ºå‡†ã€‚ä¸æ–°å¢Keyï¼Œåªç¿»è¯‘ç°æœ‰çš„ã€‚ç»“æ„ç»å¯¹å®‰å…¨ã€‚(è§£å†³è¯»ä¸å‡ºæ¥çš„é—®é¢˜)
# "SOURCE_MASTER": ä»¥æºæ–‡ä»¶(EN)ç»“æ„ä¸ºå‡†ã€‚ä¼šè‡ªåŠ¨è¡¥å…¨ç¼ºå¤±çš„Keyã€‚å¯èƒ½å¯¼è‡´ç»“æ„æ”¹å˜ã€‚
SYNC_MODE = "TARGET_MASTER" 

# 2. æ ¸å¿ƒæ–‡ä»¶é…ç½® (è‡ªåŠ¨è½¬æ¢ä¸ºPathå¯¹è±¡)
SOURCE_EN_JSON_PATH = Path("pf2e-beginner-box-en.json")  # ä»…ç”¨äºå‚è€ƒåŸæ–‡
TARGET_JSON_PATH = Path("pf2e-beginner-box.adventures.json")  # æ—¢æ˜¯ç»“æ„æ¨¡æ¿ï¼Œä¹Ÿæ˜¯è¾“å‡ºç›®æ ‡

# 3. æ¨¡å‹ä¼˜å…ˆçº§ (è‡ªåŠ¨é™çº§)
MODEL_PRIORITY_LIST = [
    ("openai", "gpt-5.2"),          # ä¼˜å…ˆçº§1
    ("openai", "gpt-5-mini"),       # ä¼˜å…ˆçº§2
    ("google", "gemini-3-flash-preview"), # ä¼˜å…ˆçº§3
]

# 4. æ€§èƒ½é…ç½®
MAX_WORKERS = 16    
TARGET_RPM = 450   
MAX_RETRIES = 5     

# 5. [æ ¸å¿ƒå¼€å…³] æš´åŠ›é˜²æ¼æ¨¡å¼ (ä»…åœ¨ç¿»è¯‘å†…å®¹åˆ¤æ–­æ—¶ç”Ÿæ•ˆ)
BRUTE_FORCE_MODE = True 

# 6. æœ¯è¯­è¡¨ä¸æ—¥å¿— (è‡ªåŠ¨è½¬æ¢ä¸ºPathå¯¹è±¡)
GLOBAL_GLOSSARY_PATH = Path("æœ¯è¯­è¯‘åå¯¹ç…§è¡¨.csv") 
LOCAL_GLOSSARY_EXPORT_PATH = Path("æœ¯è¯­è¡¨_æœ¬åœ°æå–.csv")
REPORT_XLSX_PATH = Path("ç¿»è¯‘å®¡æŸ¥æŠ¥å‘Š.xlsx")
PROCESS_LOG_PATH = Path("è¿è¡Œæ—¥å¿—.txt")
MISSED_LOG_PATH = Path("å¤±è´¥æ¼ç¿»è®°å½•.txt")
HISTORY_FILE_PATH = Path("translation_history.json")
BACKUP_DIR = Path("backups")

# 7. æ—¥å¿—è¾“å‡º
PRINT_LOG_TO_TERMINAL = True  # åŒæ­¥è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¾¿äºå®æ—¶è§‚å¯Ÿ

# ç›®æ ‡å­—æ®µç™½åå•
TARGET_KEYS = {
    "name", "description", "text", "label", "caption", "value", 
    "unidentifiedName", "tokenName", "publicnotes", "publicNotes",
    "gm_notes", "gm_description", "header", "content", "items", 
    "navName", "tooltip", "preAuthored"
}
SPECIAL_CONTAINERS = {
    "notes", "folders", "journal", "journals", "scenes", 
    "actors", "items", "pages", "entries", "flags", "system"
}

# ===========================================

# åˆå§‹åŒ–å®¢æˆ·ç«¯
google_client = None
openai_client = None

if any(p == "google" for p, m in MODEL_PRIORITY_LIST) and GOOGLE_API_KEY:
    google_client = genai.Client(api_key=GOOGLE_API_KEY)

if any(p == "openai" for p, m in MODEL_PRIORITY_LIST) and OPENAI_API_KEY:
    openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

log_lock = Lock()
report_data = {"New": [], "Fixed": [], "Kept": []}
process_log_buffer = []
missed_log_buffer = [] 
history_cache = set()
new_history_entries = set()

GOOGLE_SAFETY = [
    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
]

def write_process_log(msg):
    """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—å†™å…¥"""
    with log_lock:
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        line = f"[{timestamp}] {msg}"
        process_log_buffer.append(line)
        if PRINT_LOG_TO_TERMINAL:
            print(line)
        # æ¯100æ¡æ—¥å¿—è‡ªåŠ¨åˆ·ç›˜ä¸€æ¬¡
        if len(process_log_buffer) >= 100:
            _flush_process_log()

def _flush_process_log():
    """åˆ·æ–°è¿›ç¨‹æ—¥å¿—åˆ°æ–‡ä»¶"""
    if process_log_buffer and PROCESS_LOG_PATH:
        try:
            with PROCESS_LOG_PATH.open('a', encoding='utf-8') as f:
                f.write("\n".join(process_log_buffer) + "\n")
            process_log_buffer.clear()
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")

def write_missed_log(path, text, reason):
    """è®°å½•æ¼ç¿»æ¡ç›®"""
    with log_lock:
        missed_log_buffer.append(f"ã€{reason}ã€‘Path: {path}\nText: {text[:50]}...\n{'-'*30}")

# === AI è°ƒç”¨æ¥å£ ===
def call_ai_with_fallback(sys_prompt, user_prompt, path_str):
    """å¸¦å›é€€æœºåˆ¶çš„AIè°ƒç”¨
    
    ä¼˜å…ˆçº§é¡ºåºï¼š
    1. OpenAI GPT-5.2
    2. OpenAI GPT-5-mini  
    3. Google Gemini-3-flash-preview
    """
    last_error = None
    for provider, model_id in MODEL_PRIORITY_LIST:
        if provider == "google" and not google_client:
            continue
        if provider == "openai" and not openai_client:
            continue
        
        for attempt in range(MAX_RETRIES):
            try:
                write_process_log(f"ğŸ§  è°ƒç”¨æ¨¡å‹: {model_id} | ç¬¬{attempt+1}æ¬¡ | {path_str}")
                if provider == "google":
                    response = google_client.models.generate_content(
                        model=model_id,
                        contents=f"{sys_prompt}\n{user_prompt}",
                        config=types.GenerateContentConfig(temperature=0.1, safety_settings=GOOGLE_SAFETY)
                    )
                    if not response.text:
                        raise ValueError("Empty Google Response")
                    write_process_log(f"âœ… æ¨¡å‹å®Œæˆ: {model_id} | {path_str}")
                    return response.text
                elif provider == "openai":
                    if "gpt-5" in model_id or "o1" in model_id or "o3" in model_id:
                        # ä½¿ç”¨ Responses API
                        response = openai_client.responses.create(
                            model=model_id,
                            instructions=sys_prompt,
                            input=user_prompt,
                            reasoning={"effort": "none"}
                        )
                        write_process_log(f"âœ… æ¨¡å‹å®Œæˆ: {model_id} | {path_str}")
                        return response.output_text
                    else:
                        # ä½¿ç”¨ Chat Completions
                        response = openai_client.chat.completions.create(
                            model=model_id,
                            messages=[
                                {"role": "system", "content": sys_prompt},
                                {"role": "user", "content": user_prompt}
                            ],
                            temperature=0.1
                        )
                        write_process_log(f"âœ… æ¨¡å‹å®Œæˆ: {model_id} | {path_str}")
                        return response.choices[0].message.content
            except Exception as e:
                last_error = e
                write_process_log(f"âš ï¸ æ¨¡å‹å¤±è´¥: {model_id} | {path_str} | {e}")
                # é‡åˆ°é€Ÿç‡é™åˆ¶ç«‹å³é‡è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                if "429" in str(e) or "Resource Unavailable" in str(e):
                    break
                # å…¶ä»–é”™è¯¯ç­‰å¾…åé‡è¯•
                time.sleep(1 * (attempt + 1))
        
        write_process_log(f"âš ï¸ æ¨¡å‹ {model_id} å¤±è´¥: {last_error} -> å°è¯•ä¸‹ä¸€é¡ºä½")
    
    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    raise last_error

# === åŸºç¡€å·¥å…· ===
def backup_existing_files():
    """å¤‡ä»½ç°æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•"""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    files = [TARGET_JSON_PATH, REPORT_XLSX_PATH, LOCAL_GLOSSARY_EXPORT_PATH, HISTORY_FILE_PATH]
    for file_path in files:
        if file_path.exists():
            try:
                backup_path = BACKUP_DIR / f"{timestamp}_{file_path.name}.bak"
                backup_path.write_bytes(file_path.read_bytes())
                write_process_log(f"âœ… å¤‡ä»½å®Œæˆ: {file_path.name} -> {backup_path}")
            except Exception as e:
                write_process_log(f"å¤‡ä»½å¤±è´¥: {file_path.name} - {e}")

def get_content_hash(en, cn):
    return hashlib.md5(f"{en or ''}::{cn or ''}".encode('utf-8')).hexdigest()

def load_history():
    """ä»å†å²æ–‡ä»¶åŠ è½½å·²å¤„ç†é¡¹ç›®çš„å“ˆå¸Œå€¼"""
    if not HISTORY_FILE_PATH.exists():
        return set()
    try:
        with HISTORY_FILE_PATH.open('r', encoding='utf-8') as f:
            return set(json.load(f))
    except Exception as e:
        write_process_log(f"âš ï¸ åŠ è½½å†å²æ–‡ä»¶å¤±è´¥: {e}")
        return set()

def save_history():
    """å°†ç¼“å­˜å’Œæ–°æ¡ç›®ä¿å­˜åˆ°å†å²æ–‡ä»¶"""
    try:
        with HISTORY_FILE_PATH.open('w', encoding='utf-8') as f:
            json.dump(list(history_cache.union(new_history_entries)), f)
    except Exception as e:
        write_process_log(f"âš ï¸ ä¿å­˜å†å²æ–‡ä»¶å¤±è´¥: {e}")

class RateLimiter:
    def __init__(self, rpm):
        self.interval = 60.0 / rpm
        self.last_dispatch_time = 0
    def wait_for_slot(self):
        now = time.time()
        wait = self.last_dispatch_time + self.interval - now
        if wait > 0: time.sleep(wait)
        self.last_dispatch_time = time.time()

class CodeProtector:
    def __init__(self):
        self.patterns = [
            re.compile(r'(@[a-zA-Z0-9]+\[[^\]]*\])'),
            re.compile(r'(\[\[.*?\]\])'),
            re.compile(r'(<[^>]+>)'),
            re.compile(r'(&[a-zA-Z0-9#]+;)'),
        ]
    def mask(self, text):
        if not text: return text, {}
        ph, ctr = {}, 0
        def repl(m):
            nonlocal ctr
            k = f"__CODE_{ctr}__"
            ph[k] = m.group(1)
            ctr += 1
            return k
        for p in self.patterns: text = p.sub(repl, text)
        return text, ph
    def unmask(self, text, ph):
        if not text: return text
        for k, v in ph.items():
            text = text.replace(k, v)
            if k not in text: text = re.sub(k.replace('_', r'\s*_\s*'), v, text)
        return text

class GlossaryManager:
    """æœ¯è¯­è¡¨ç®¡ç†å™¨
    
    è´Ÿè´£åŠ è½½æœ¯è¯­è¡¨ã€åŒ¹é…å’Œæ³¨å…¥æœ¯è¯­åˆ°æ–‡æœ¬
    æ”¯æŒå¤šç§ç¼–ç çš„CSVæ–‡ä»¶
    """
    
    def __init__(self, global_csv, local_csv=None):
        """åˆå§‹åŒ–æœ¯è¯­è¡¨
        
        Args:
            global_csv: å…¨å±€æœ¯è¯­è¡¨è·¯å¾„
            local_csv: æœ¬åœ°æœ¯è¯­è¡¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.term_map = {}  # æœ¯è¯­æ˜ å°„ {è‹±æ–‡: {target, org, re}}
        self.sorted_keys = []
        self.load_glossary(global_csv)
        if local_csv:
            local_csv = Path(local_csv)
            if local_csv.exists():
                self.load_glossary(local_csv)
        # æŒ‰é•¿åº¦é™åºæ’åºï¼Œé˜²æ­¢çŸ­è¯å…ˆåŒ¹é…
        self.sorted_keys = sorted(self.term_map.keys(), key=lambda x: len(x), reverse=True)
        print(f"æœ¯è¯­åº“åŠ è½½: {len(self.sorted_keys)} æ¡")

    def load_glossary(self, path):
        """åŠ è½½CSVæœ¯è¯­è¡¨ï¼Œæ”¯æŒå¤šç§ç¼–ç """
        path = Path(path)
        if not path.exists():
            return
        df = None
        for enc in ['utf-8', 'utf-8-sig', 'gbk']:
            try:
                df = pd.read_csv(path, encoding=enc)
                break
            except Exception:
                continue
        if df is None:
            return
        for r in df.to_dict('records'):
            cn, en = str(r.get('Target', '')).strip(), str(r.get('Source', '')).strip()
            if cn and en and en.lower() != 'nan':
                flags = 0 if any(c.isupper() for c in en) else re.IGNORECASE
                self.term_map[en] = {"target": cn, "org": en, "re": re.compile(r'\b' + re.escape(en) + r'\b', flags)}

    def pre_inject_text(self, text, path_str):
        """åœ¨æ–‡æœ¬ä¸­æ³¨å…¥æœ¯è¯­æ ‡è®°ï¼Œä¾›AIè¯†åˆ«
        
        Args:
            text: å¾…å¤„ç†æ–‡æœ¬
            path_str: JSONè·¯å¾„ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Returns:
            (æ³¨å…¥åçš„æ–‡æœ¬, æ³¨å…¥çš„æœ¯è¯­åˆ—è¡¨)
        """
        if not text:
            return text, []
        
        inj, ph, idx = [], {}, 0
        # å¿«é€Ÿè¿‡æ»¤ï¼šåªæ£€æŸ¥æ–‡æœ¬ä¸­å®é™…åŒ…å«çš„è¯æ±‡
        tokens = set(re.findall(r'[a-z]+', text.lower()))
        cands = [k for k in self.sorted_keys if (k.lower() in tokens) or (" " in k and k.lower() in text.lower())]
        
        for k in cands:
            d = self.term_map[k]
            matches = list(d["re"].finditer(text))
            if matches:
                def repl(m):
                    nonlocal idx
                    mt = m.group(0)
                    # ä¿æŒåŸæœ‰çš„å¤§å°å†™æ ·å¼
                    if mt == d["org"] or (d["org"].islower() and mt.istitle()):
                        inj.append((d["org"], d["target"]))
                        k_ph = f"__Tm_{idx}__"
                        ph[k_ph] = f"âŸª{d['target']}|åŸæ–‡:{mt}âŸ«"
                        idx += 1
                        return k_ph
                    return mt
                text = d["re"].sub(repl, text)
        
        # å°†å ä½ç¬¦æ›¿æ¢ä¸ºæ ‡è®°
        for k, v in ph.items():
            text = text.replace(k, v)
        
        return text, inj

def smart_format_bilingual(cn, en):
    """æ™ºèƒ½æ ¼å¼åŒ–åŒè¯­æ–‡æœ¬
    
    Args:
        cn: ä¸­æ–‡æ–‡æœ¬
        en: è‹±æ–‡åŸæ–‡
    
    Returns:
        æ ¼å¼åŒ–åçš„æ–‡æœ¬ï¼ˆæŒ‰éœ€æ·»åŠ åŸæ–‡ï¼‰
    """
    if not cn:
        return en
    # æ¸…ç†æ³¨å…¥æ ‡ç­¾
    cn = re.sub(r'âŸª(.*?)\|åŸæ–‡:.*?âŸ«', r'\1', cn)
    # æ£€æŸ¥ä¸­æ–‡æ˜¯å¦å·²åŒ…å«è‹±æ–‡å†…å®¹
    clean_en = re.sub(r'[\s\W]', '', en).lower()
    clean_cn = re.sub(r'[\s\W]', '', cn).lower()
    if clean_en in clean_cn:
        return cn
    # çŸ­æ–‡æœ¬ï¼šå¦‚æœè¯‘æ–‡é‡Œå·²åŒ…å«è‹±æ–‡ï¼Œé¿å…å†æ¬¡è¿½åŠ åŸæ–‡å¯¼è‡´é‡å¤
    if en and len(en) <= 80 and re.search(r'[A-Za-z]', cn):
        return cn
    # çŸ­æ–‡æœ¬ï¼šå¦‚æœè¯‘æ–‡å·²åŒ…å«åŸæ–‡ä¸­çš„ä»»ä¸€å…³é”®è‹±æ–‡è¯ï¼Œä¹Ÿè§†ä¸ºå·²å«åŸæ–‡
    if en and len(en) <= 80:
        for w in re.findall(r"[A-Za-z][A-Za-z']{2,}", en):
            if re.search(rf"\b{re.escape(w)}\b", cn, flags=re.IGNORECASE):
                return cn
    # é•¿æ–‡æœ¬ç”¨æ¢è¡Œåˆ†éš”ï¼ŒçŸ­æ–‡æœ¬ç”¨ç©ºæ ¼
    sep = "<br><br><hr><b>åŸæ–‡:</b><br>" if (len(en) > 80 or "<p>" in en) else " "
    return f"{cn}{sep}{en}"

def extract_local_glossary(en_data, cn_data, output_path):
    """ä»ç¿»è¯‘æ•°æ®ä¸­æå–æœ¬åœ°æœ¯è¯­è¡¨"""
    print("æ­£åœ¨æ‰«ææœ¬åœ°æœ¯è¯­...")
    extracted = []
    
    def traverse(en_node, cn_node):
        """é€’å½’éå†æ•°æ®ç»“æ„ï¼Œæå–å·²ç¿»è¯‘é¡¹"""
        if isinstance(en_node, dict) and isinstance(cn_node, dict):
            for k, v in en_node.items():
                if k in cn_node:
                    traverse(v, cn_node[k])
        elif isinstance(en_node, list) and isinstance(cn_node, list):
            for i in range(min(len(en_node), len(cn_node))):
                traverse(en_node[i], cn_node[i])
        elif isinstance(en_node, str) and isinstance(cn_node, str):
            if len(en_node) < 60 and len(cn_node) > 0 and en_node != cn_node:
                clean_cn = smart_format_bilingual(cn_node, "")
                if clean_cn and re.search(r'[\u4e00-\u9fff]', clean_cn):
                    extracted.append({'Source': en_node, 'Target': clean_cn})
    
    traverse(en_data, cn_data)
    if extracted:
        try:
            df = pd.DataFrame(extracted).drop_duplicates(subset=['Source'])
            output_path = Path(output_path)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
        except Exception as e:
            write_process_log(f"âš ï¸ å¯¼å‡ºæœ¯è¯­è¡¨å¤±è´¥: {e}")

def clean_response_text(text):
    """æ¸…ç†AIå“åº”æ–‡æœ¬"""
    if not text:
        return ""
    text = re.sub(r'^```[a-zA-Z]*\n', '', text)
    text = re.sub(r'\n```$', '', text)
    text = re.sub(r'^(Here is|Below is|ä»¥ä¸‹æ˜¯).*?(\n|$)', '', text, flags=re.IGNORECASE).strip()
    return text.strip()

def cleanup_injection_tags(text):
    """æ¸…ç†æœ¯è¯­æ³¨å…¥æ ‡ç­¾"""
    if not text:
        return text
    # ç§»é™¤æ³¨å…¥æ ‡ç­¾ï¼Œä¿ç•™ç¿»è¯‘
    text = re.sub(r'âŸª(.*?)\|åŸæ–‡:.*?âŸ«', r'\1', text)
    return text

def collapse_duplicate_cn_prefix(text):
    """æ¸…ç†çŸ­æ–‡æœ¬å¼€å¤´çš„é‡å¤ä¸­æ–‡è¯ç»„

    ä¾‹: "å±æ€§å€¼ å±æ€§å€¼ Ability Scores" -> "å±æ€§å€¼ Ability Scores"
    """
    if not text:
        return text
    parts = text.split()
    if len(parts) < 2:
        return text
    if parts[0] == parts[1] and re.search(r'[\u4e00-\u9fff]', parts[0]):
        i = 1
        while i < len(parts) and parts[i] == parts[0]:
            i += 1
        return " ".join([parts[0]] + parts[i:])
    return text

protector = CodeProtector()

def process_single_item(task_type, en_text, cn_draft, glossary_mgr, path_str):
    """å¤„ç†å•ä¸ªç¿»è¯‘é¡¹
    
    Args:
        task_type: 'NEW' æˆ– 'AUDIT'
        en_text: è‹±æ–‡åŸæ–‡
        cn_draft: ä¸­æ–‡åˆç¨¿ï¼ˆå®¡æ ¸æ¨¡å¼ï¼‰
        glossary_mgr: æœ¯è¯­è¡¨ç®¡ç†å™¨
        path_str: JSONè·¯å¾„
    
    Returns:
        (ç¿»è¯‘ç»“æœ, çŠ¶æ€)
    """
    # åŸºç¡€æ£€æŸ¥
    if not en_text or len(en_text) < 2:
        return en_text, None
    if not re.search(r'[a-zA-Z]', en_text):
        return en_text, None

    write_process_log(f"ğŸ§© å¤„ç†ä»»åŠ¡: {task_type} | {path_str}")
    
    # ä»£ç ä¿æŠ¤å’Œæœ¯è¯­æ³¨å…¥
    prot = CodeProtector()
    masked, code_ph = prot.mask(en_text)
    injected, terms = glossary_mgr.pre_inject_text(masked, path_str)
    
    # æ¸…ç†åˆç¨¿
    clean_draft_txt = cn_draft
    if cn_draft and "<hr>" in cn_draft:
        clean_draft_txt = cn_draft.split("<hr>")[0].strip()
    
    # æ„å»ºæç¤ºè¯
    sys_prompt = "You are a professional Pathfinder 2e translator. Output ONLY Chinese. Keep HTML/Codes."
    if task_type == "AUDIT":
        prompt = (
            f"Original:\n```\n{injected}\n```\nDraft:\n```\n{clean_draft_txt}\n```\n"
            f"Task: Review draft. If correct, output it. If wrong, correct it."
        )
    else:
        prompt = f"Translate:\n```\n{injected}\n```"

    try:
        res_text = call_ai_with_fallback(sys_prompt, prompt, path_str)
        trans = clean_response_text(res_text)
        final_trans = prot.unmask(trans, code_ph)
        final_trans = cleanup_injection_tags(final_trans)
        final_trans = collapse_duplicate_cn_prefix(final_trans)

        # ç¡®å®šçŠ¶æ€
        status = "New"
        if task_type == "AUDIT":
            # å¯¹æ¯”å‰åå»ç©ºæ ¼
            if re.sub(r'\s', '', final_trans) == re.sub(r'\s', '', clean_draft_txt):
                status = "Kept"
            else:
                status = "Fixed"

        log_report(status, path_str, en_text, final_trans, terms)
        write_process_log(f"âœ… ä»»åŠ¡å®Œæˆ: {status} | {path_str}")
        return smart_format_bilingual(final_trans, en_text), status

    except Exception as e:
        write_process_log(f"âŒ æ‰€æœ‰æ¨¡å‹å¤±è´¥ {path_str}: {e}")
        write_missed_log(path_str, en_text, "All Models Failed")
        fallback = smart_format_bilingual(cn_draft, en_text) if cn_draft else f"ã€FAILã€‘{en_text}"
        return fallback, None

def log_report(status, path, original, translated, injected_terms):
    term_str = " | ".join([f"{e}->{c}" for e, c in injected_terms])
    row = {"JSON Path": path, "Involved Terms": term_str, "Original": original, "Translation": translated}
    with log_lock:
        if status in report_data:
            report_data[status].append(row)

# === V32 æ ¸å¿ƒï¼šä»»åŠ¡æ”¶é›†åˆ†æµ ===

def collect_tasks_source_master(en_data, cn_data, path_str="root"):
    """
    [æ—§é€»è¾‘] ä»¥ Source (è‹±æ–‡) ä¸ºä¸»ã€‚
    å¦‚æœ Source æœ‰ä½† Target æ²¡æœ‰ï¼Œä¼šæ–°å¢ï¼ˆå¯èƒ½ç ´å Target ç»“æ„ï¼‰ã€‚
    """
    tasks = []
    
    def get_cn(d, k):
        if isinstance(d, dict): return d.get(k)
        if isinstance(d, list) and isinstance(k, int) and k < len(d): return d[k]
        return None

    if isinstance(en_data, dict): iter_items = en_data.items()
    elif isinstance(en_data, list): iter_items = enumerate(en_data)
    else: return []

    for k, v in iter_items:
        cur_path = f"{path_str}.{k}" if isinstance(en_data, dict) else f"{path_str}[{k}]"
        cn_v = get_cn(cn_data, k)
        
        # åˆ¤æ–­é€»è¾‘
        should_translate = False
        if isinstance(v, str) and len(v) > 1:
            has_letters = bool(re.search(r'[a-zA-Z]', v))
            if has_letters:
                is_target_key = False
                if isinstance(en_data, dict) and k in TARGET_KEYS: is_target_key = True
                elif any(c in path_str.split('.') for c in SPECIAL_CONTAINERS): is_target_key = True
                is_file = v.lower().endswith(('.png', '.webp', '.jpg', '.mp3', '.ogg', '.m4a', '.webm'))
                has_space = " " in v
                if BRUTE_FORCE_MODE:
                    if not is_file and (is_target_key or has_space): should_translate = True
                else:
                    if is_target_key: should_translate = True

        if should_translate:
            if cn_v and get_content_hash(v, cn_v) in history_cache: continue
            tt = 'AUDIT' if (cn_v and isinstance(cn_v, str) and len(cn_v) > 0 and cn_v != v) else 'NEW'
            tasks.append({'type': tt, 'ref': en_data, 'k': k, 'en_v': v, 'cn_v': cn_v if tt=='AUDIT' else None, 'path': cur_path})
            
        elif isinstance(v, (dict, list)):
            new_cn = cn_v if isinstance(cn_v, (dict, list)) else {}
            tasks.extend(collect_tasks_source_master(v, new_cn, cur_path))
            
    return tasks

def collect_tasks_target_master(cn_data, en_data, path_str="root"):
    """
    [V32 æ–°é€»è¾‘] ä»¥ Target (ä¸­æ–‡) ä¸ºä¸»ã€‚
    åªéå† Target çš„ç»“æ„ã€‚å¦‚æœ Target é‡Œæœ‰è‹±æ–‡ï¼Œå°±ç¿»è¯‘ã€‚
    å®Œå…¨å¿½ç•¥ Source ä¸­å¤šå‡ºæ¥çš„ç»“æ„ï¼ˆDumpæ‰ï¼‰ã€‚
    """
    tasks = []
    
    # è¾…åŠ©å‡½æ•°ï¼šå°è¯•åœ¨ Source æ•°æ®é‡Œæ‰¾åˆ°å¯¹åº”çš„è·¯å¾„ï¼Œä»¥è·å–æœ€çº¯æ­£çš„åŸæ–‡ï¼ˆç”¨äºå‚è€ƒï¼‰
    def get_en_counterpart(source_node, key):
        if source_node is None: return None
        if isinstance(source_node, dict): return source_node.get(key)
        if isinstance(source_node, list) and isinstance(key, int) and key < len(source_node): return source_node[key]
        return None

    if isinstance(cn_data, dict): iter_items = cn_data.items()
    elif isinstance(cn_data, list): iter_items = enumerate(cn_data)
    else: return []

    for k, v in iter_items:
        cur_path = f"{path_str}.{k}" if isinstance(cn_data, dict) else f"{path_str}[{k}]"
        
        # å°è¯•å» Source é‡Œæ‰¾å¯¹åº”çš„åŸæ–‡
        # å¦‚æœç»“æ„ä¸åŒ¹é…ï¼ˆTargetç»“æ„æµ…ï¼ŒSourceç»“æ„æ·±ï¼‰ï¼Œen_v å¯èƒ½æ˜¯ None
        en_v = get_en_counterpart(en_data, k)
        
        # åˆ¤æ–­é€»è¾‘
        should_translate = False
        # æˆ‘ä»¬æ£€æŸ¥ v (Targeté‡Œçš„å€¼)ã€‚å¦‚æœå®ƒæ˜¯å­—ç¬¦ä¸²ï¼Œä¸”åŒ…å«è‹±æ–‡ï¼Œæˆ‘ä»¬å°±å¾—ç¿»ã€‚
        # ä¸ç®¡å®ƒæ˜¯ä¸æ˜¯åœ¨ç™½åå•é‡Œï¼Œåªè¦åœ¨ Target æ–‡ä»¶é‡Œå‡ºç°äº†ï¼Œé€šå¸¸éƒ½æ„å‘³ç€è¦ä¿ç•™ã€‚
        if isinstance(v, str) and len(v) > 1:
            has_letters = bool(re.search(r'[a-zA-Z]', v))
            if has_letters:
                # ä¾ç„¶åº”ç”¨ä¸€äº›åŸºç¡€è¿‡æ»¤ï¼Œé˜²æ­¢ç¿»è¯‘ ID æˆ– è·¯å¾„
                is_file = v.lower().endswith(('.png', '.webp', '.jpg', '.mp3', '.ogg', '.m4a', '.webm'))
                is_target_key = False
                if isinstance(cn_data, dict) and k in TARGET_KEYS: is_target_key = True
                
                # åœ¨ TargetMaster æ¨¡å¼ä¸‹ï¼Œå¦‚æœ key åœ¨ç™½åå•é‡Œï¼Œæˆ–è€…çœ‹èµ·æ¥åƒå¥å­ï¼ˆæœ‰ç©ºæ ¼ï¼‰ï¼Œå°±ç¿»
                if not is_file and (is_target_key or " " in v):
                     should_translate = True

        if should_translate:
            # å¦‚æœ Source é‡Œæ‰¾ä¸åˆ°å¯¹åº”çš„ en_v (å› ä¸ºç»“æ„ä¸åŒ)ï¼Œæˆ‘ä»¬å°±æŠŠå½“å‰ Target é‡Œçš„ v å½“ä½œåŸæ–‡
            original_text = en_v if (en_v and isinstance(en_v, str)) else v
            
            # æ£€æŸ¥ç¼“å­˜
            if get_content_hash(original_text, v) in history_cache: continue
            
            # å¦‚æœ v å·²ç»æ˜¯ä¸­æ–‡äº†ï¼Œæˆ–è€…å«ä¸­æ–‡ï¼Œæˆ‘ä»¬æ ‡è®°ä¸º AUDITï¼›å¦‚æœæ˜¯çº¯è‹±æ–‡ï¼Œæ ‡è®°ä¸º NEW
            is_translated = bool(re.search(r'[\u4e00-\u9fff]', v))
            task_type = 'AUDIT' if is_translated else 'NEW'
            
            # æ³¨æ„ï¼šè¿™é‡Œçš„ ref æ˜¯ cn_dataï¼Œå› ä¸ºæˆ‘ä»¬è¦å›å†™åˆ° Target
            tasks.append({
                'type': task_type,
                'ref': cn_data, 
                'k': k,
                'en_v': original_text, # é€ç»™ AI çš„å‚è€ƒåŸæ–‡
                'cn_v': v,             # é€ç»™ AI çš„ç°æœ‰è¯‘æ–‡ï¼ˆç”¨äºæ ¡å¯¹ï¼‰
                'path': cur_path
            })
            
        elif isinstance(v, (dict, list)):
            # é€’å½’æ—¶ï¼Œen_v å¯èƒ½æ˜¯ None (å¦‚æœç»“æ„å¯¹ä¸ä¸Š)ï¼Œè¿™æ²¡å…³ç³»ï¼Œç»§ç»­å¾€ä¸‹ä¼  None å³å¯
            tasks.extend(collect_tasks_target_master(v, en_v, cur_path))
            
    return tasks

def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒå’ŒAPIé…ç½®"""
    issues = []
    
    # æ£€æŸ¥APIå¯†é’¥
    if not GOOGLE_API_KEY and not OPENAI_API_KEY:
        issues.append("âŒ æœªé…ç½®ä»»ä½•APIå¯†é’¥ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡: GOOGLE_API_KEY æˆ– OPENAI_API_KEY")
    
    if GOOGLE_API_KEY and not google_client:
        issues.append("âš ï¸ Google APIå¯†é’¥é…ç½®å¤±è´¥ï¼Œè·³è¿‡GoogleæœåŠ¡")
    
    if OPENAI_API_KEY and not openai_client:
        issues.append("âš ï¸ OpenAI APIå¯†é’¥é…ç½®å¤±è´¥ï¼Œè·³è¿‡OpenAIæœåŠ¡")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not SOURCE_EN_JSON_PATH.exists():
        issues.append(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {SOURCE_EN_JSON_PATH}")
    
    if SYNC_MODE == "TARGET_MASTER" and not TARGET_JSON_PATH.exists():
        issues.append(f"âŒ TARGET_MASTERæ¨¡å¼éœ€è¦ç›®æ ‡æ–‡ä»¶: {TARGET_JSON_PATH}")
    
    # è¾“å‡ºæ£€æŸ¥ç»“æœ
    for issue in issues:
        print(issue)
    
    # å¦‚æœæœ‰å…³é”®é”™è¯¯åˆ™è¿”å›False
    return len([i for i in issues if i.startswith("âŒ")]) == 0

def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•´ä¸ªç¿»è¯‘æµç¨‹"""
    print(f"PF2e æ±‰åŒ–è„šæœ¬ V32 (åŒæ­¥æ¨¡å¼: {SYNC_MODE})")
    print(f"æºæ–‡ä»¶: {SOURCE_EN_JSON_PATH.name if SOURCE_EN_JSON_PATH.exists() else '(ä¸å­˜åœ¨)'}")
    print(f"ç›®æ ‡æ–‡ä»¶: {TARGET_JSON_PATH.name if TARGET_JSON_PATH.exists() else '(ä¸å­˜åœ¨)'}")
    print("")
    
    # ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        return

    # å¤‡ä»½ç°æœ‰æ–‡ä»¶
    backup_existing_files()
    global history_cache
    history_cache = load_history()
    print(f"ğŸ§  å·²åŠ è½½ç¼“å­˜: {len(history_cache)} æ¡è®°å½•")
    write_process_log(f"ç¼“å­˜æ¡ç›®: {len(history_cache)}")

    # åŠ è½½æºæ–‡ä»¶
    with SOURCE_EN_JSON_PATH.open('r', encoding='utf-8-sig') as f:
        en_data = json.load(f)
    
    # åŠ è½½æˆ–åˆå§‹åŒ–ç›®æ ‡æ–‡ä»¶
    cn_data = {}
    if TARGET_JSON_PATH.exists():
        try:
            print("ğŸ”„ åŠ è½½ç›®æ ‡æ–‡ä»¶...")
            with TARGET_JSON_PATH.open('r', encoding='utf-8-sig') as f:
                cn_data = json.load(f)
            write_process_log(f"ç›®æ ‡æ–‡ä»¶åŠ è½½æˆåŠŸ: {TARGET_JSON_PATH}")
        except Exception as e:
            print(f"âŒ åŠ è½½ç›®æ ‡æ–‡ä»¶å¤±è´¥: {e}")
            if SYNC_MODE == "TARGET_MASTER":
                print("â›” åœ¨ TARGET_MASTER æ¨¡å¼ä¸‹ï¼Œç›®æ ‡æ–‡ä»¶å¿…é¡»å­˜åœ¨ä¸”æœ‰æ•ˆï¼")
                return
    else:
        if SYNC_MODE == "TARGET_MASTER":
            print("â›” é”™è¯¯ï¼šTARGET_MASTER æ¨¡å¼éœ€è¦ç›®æ ‡æ–‡ä»¶å­˜åœ¨ (ä½œä¸ºç»“æ„æ¨¡æ¿)ã€‚")
            return
        # Source Master æ¨¡å¼å¯ä»¥ä»ç©ºå¼€å§‹
        cn_data = {}

    extract_local_glossary(en_data, cn_data, LOCAL_GLOSSARY_EXPORT_PATH)
    glossary = GlossaryManager(GLOBAL_GLOSSARY_PATH, LOCAL_GLOSSARY_EXPORT_PATH)
    write_process_log(f"æœ¯è¯­åº“åŠ è½½å®Œæˆ: {len(glossary.sorted_keys)} æ¡")
    
    print("æ„å»ºä»»åŠ¡é˜Ÿåˆ—...")
    
    # === V32 åˆ†æµé€»è¾‘ ===
    if SYNC_MODE == "TARGET_MASTER":
        # éå† Targetï¼Œå¿½ç•¥ Source å¤šä½™ç»“æ„
        all_tasks = collect_tasks_target_master(cn_data, en_data)
    else:
        # éå† Sourceï¼Œå¼ºåˆ¶è¡¥å…¨ Target
        all_tasks = collect_tasks_source_master(en_data, cn_data)
        
    print(f"å¾…å¤„ç†ä»»åŠ¡: {len(all_tasks)}")
    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹
    type_counts = {"NEW": 0, "AUDIT": 0}
    for t in all_tasks:
        if t["type"] in type_counts:
            type_counts[t["type"]] += 1
    write_process_log(f"ä»»åŠ¡ç»Ÿè®¡: NEW={type_counts['NEW']}, AUDIT={type_counts['AUDIT']}")
    
    if not all_tasks: 
        print("ğŸ‰ æ²¡æœ‰éœ€è¦æ›´æ–°çš„å†…å®¹ï¼")
        return

    rl = RateLimiter(TARGET_RPM)
    print("ğŸš€ å¼•æ“å¯åŠ¨...")
    write_process_log(f"çº¿ç¨‹æ± å¯åŠ¨: workers={MAX_WORKERS}, RPM={TARGET_RPM}")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ä»»åŠ¡
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
        fut_map = {}
        # æäº¤ä»»åŠ¡
        for t in tqdm(all_tasks, desc="åˆ†å‘"):
            rl.wait_for_slot()
            future = exe.submit(process_single_item, t['type'], t['en_v'], t['cn_v'], glossary, t['path'])
            fut_map[future] = t
        
        # æ”¶é›†ç»“æœ
        for f in tqdm(concurrent.futures.as_completed(fut_map), total=len(all_tasks), desc="å›æ”¶"):
            task = fut_map[f]
            try:
                res, st = f.result(timeout=30)  # å•ä¸ªä»»åŠ¡è¶…æ—¶30ç§’
                task['ref'][task['k']] = res
                # å¦‚æœç¿»è¯‘æœªæ”¹å˜ï¼ŒåŠ å…¥ç¼“å­˜ä»¥åŠ å¿«ä¸‹æ¬¡è¿è¡Œ
                if st == "Kept":
                    with log_lock:
                        new_history_entries.add(get_content_hash(task['en_v'], res))
            except concurrent.futures.TimeoutError:
                write_process_log(f"âŒ ä»»åŠ¡è¶…æ—¶: {task['path']}")
            except Exception as e:
                write_process_log(f"âŒ ä»»åŠ¡å¤±è´¥: {task['path']} - {e}")

    print("æ­£åœ¨ä¿å­˜æœ€ç»ˆç»“æœ...")
    # Target Master æ¨¡å¼ï¼šä¿æŒTargetç»“æ„ï¼Œåªæ›´æ–°å€¼
    # Source Master æ¨¡å¼ï¼šä½¿ç”¨Sourceç»“æ„ï¼Œå¼ºåˆ¶è¡¥å…¨Target
    output_obj = cn_data if SYNC_MODE == "TARGET_MASTER" else en_data
    
    # ä¿å­˜ç¿»è¯‘ç»“æœ
    with TARGET_JSON_PATH.open('w', encoding='utf-8') as f:
        json.dump(output_obj, f, ensure_ascii=False, indent=2)
    write_process_log(f"å†™å…¥ç›®æ ‡æ–‡ä»¶: {TARGET_JSON_PATH}")
    
    # ä¿å­˜é—æ¼æ—¥å¿—
    if missed_log_buffer:
        with MISSED_LOG_PATH.open('w', encoding='utf-8') as f:
            f.write("\n".join(missed_log_buffer))
        print(f"âš ï¸ è­¦å‘Šï¼šæœ‰ {len(missed_log_buffer)} æ¡å†…å®¹æ¼ç¿»")
    
    # ä¿å­˜å®¡æŸ¥æŠ¥å‘Šï¼ˆæ”¯æŒé‡è¯•ï¼‰
    max_retry = 3
    for attempt in range(max_retry):
        try:
            with pd.ExcelWriter(REPORT_XLSX_PATH) as w:
                pd.DataFrame(report_data["New"]).to_excel(w, sheet_name="New", index=False)
                pd.DataFrame(report_data["Fixed"]).to_excel(w, sheet_name="Fixed", index=False)
                pd.DataFrame(report_data["Kept"]).to_excel(w, sheet_name="Kept", index=False)
            break
        except PermissionError:
            if attempt < max_retry - 1:
                input(f"âŒ è¯·å…³é—­ {REPORT_XLSX_PATH} åå›è½¦...")
            else:
                print(f"âš ï¸ æ— æ³•ä¿å­˜æŠ¥å‘Šï¼Œæ–‡ä»¶è¢«å ç”¨")
        except Exception as e:
            write_process_log(f"âš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            break

    save_history()
    write_process_log("å†å²ç¼“å­˜å·²ä¿å­˜")
    # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒº
    _flush_process_log()
    write_process_log("æ—¥å¿—å·²åˆ·æ–°")
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“Š æœ¬æ¬¡è¿è¡Œç»Ÿè®¡:")
    print(f"   æ–°å¢ç¿»è¯‘: {len(report_data['New'])} é¡¹")
    print(f"   ä¿®å¤ç¿»è¯‘: {len(report_data['Fixed'])} é¡¹")
    print(f"   ä¿æŒä¸å˜: {len(report_data['Kept'])} é¡¹")
    if missed_log_buffer:
        print(f"   âš ï¸ æ¼ç¿»é¡¹ç›®: {len(missed_log_buffer)} é¡¹")
    print("="*50)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆã€‚")

if __name__ == "__main__":
    main()